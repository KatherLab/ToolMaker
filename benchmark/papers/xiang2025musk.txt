A vision–language foundation model for precision oncology

Jinxi Xiang, Xiyue Wang, Xiaoming Zhang, Yinghua Xi, Feyisope Eweje, Yijiang Chen, Yuchen Li, Colin Bergstrom, Matthew Gopaulchan, Ted Kim, Kun-Hsing Yu, Sierra Willens, Francesca Maria Olguin, Jeffrey J. Nirschl, Joel Neal, Maximilian Diehn, Sen Yang & Ruijiang Li 
Nature (2025)Cite this article

6996 Accesses
1 Citations
168 Altmetric
Metrics details
Abstract
Clinical decision-making is driven by multimodal data, including clinical notes and pathological characteristics. Artificial intelligence approaches that can effectively integrate multimodal data hold significant promise in advancing clinical care1,2. However, the scarcity of well-annotated multimodal datasets in clinical settings has hindered the development of useful models. In this study, we developed the Multimodal transformer with Unified maSKed modeling (MUSK), a vision–language foundation model designed to leverage large-scale, unlabelled, unpaired image and text data. MUSK was pretrained on 50 million pathology images from 11,577 patients and one billion pathology-related text tokens using unified masked modelling. It was further pretrained on one million pathology image–text pairs to efficiently align the vision and language features. With minimal or no further training, MUSK was tested in a wide range of applications and demonstrated superior performance across 23 patch-level and slide-level benchmarks, including image-to-text and text-to-image retrieval, visual question answering, image classification and molecular biomarker prediction. Furthermore, MUSK showed strong performance in outcome prediction, including melanoma relapse prediction, pan-cancer prognosis prediction and immunotherapy response prediction in lung and gastro-oesophageal cancers. MUSK effectively combined complementary information from pathology images and clinical reports and could potentially improve diagnosis and precision in cancer therapy.

Similar content being viewed by others

In-context learning enables multimodal large language models to classify cancer pathology images

Article Open access
21 November 2024

A visual-language foundation model for computational pathology

Article 19 March 2024

Aligning knowledge concepts to whole slide images for precise histopathology image analysis

Article Open access
30 December 2024
Main
Clinical decision-making is a complex process that involves information obtained from multiple data modalities. In clinical practice, physicians do not rely on a single data source to make diagnosis and treatment decisions. Instead, they incorporate information from multiple sources, including patient demographics, medical history, imaging findings and the pathological characteristics of the disease. Therefore, making accurate diagnosis and treatment decisions requires the synthesis of information from multimodal data. Given the complexity of these tasks, artificial intelligence (AI) approaches that can effectively integrate multimodal data hold significant promise to advance clinical care1,2,3,4,5.

Foundation models represent a new frontier in medical AI research and development6,7. These models are pretrained on massive, diverse datasets and can be applied to numerous downstream tasks with minimal or no further training8,9,10,11,12,13. This has significant advantages over the traditional approach, which requires training a new model for every new task. However, a major hurdle in the development of multimodal AI models is the scarcity of well-annotated datasets, especially in the clinical setting.

Recent efforts have been made to develop vision–language foundation models for medicine14, particularly in the field of pathology15,16,17. Although the initial results are promising, several important considerations could limit their potential clinical impact. First, these studies used off-the-shelf foundation models based on contrastive learning18, which requires paired image–text data for pretraining. Although the scale of data is impressive with approximately 0.2–1.2 million image–text pairs, it is still far below the billions of data points used for training natural vision–language models19. In addition, it remains unclear whether this scale is sufficient to fully capture the diversity of the entire disease spectrum. Second, previous studies have focused on relatively simple tasks, such as image classification or image and text retrieval, with the intended applications for cancer detection and diagnosis. However, prediction of treatment response and outcomes using multimodal foundation models has not yet been demonstrated. This is a much more challenging problem but has significant implications for guiding treatment decisions in precision medicine20.

Here, we present a new vision–language foundation model based on Multimodal transformer with Unified maSKed modeling (MUSK) for pretraining. Motivated by the success of multimodal learning of natural image–text data21, we introduce pathology-specific and general methodology adaptations to the MUSK approach for building high-performance foundation models for precision oncology. MUSK pretraining leverages large-scale, unlabelled, unpaired data with 50 million pathology images and one billion text tokens (Fig. 1a). The pathology images used for masked pretraining originated from 11,577 patients, representing 33 tumour types. We performed extensive evaluation on a wide range of downstream tasks, including image and text retrieval, visual question answering (VQA), image classification and molecular biomarker prediction. MUSK achieved superior performance over the state-of-the-art foundation models on 23 patch-level and slide-level benchmarks (Fig. 1b). Furthermore, MUSK was evaluated on multimodal clinical reports and image data from more than 8,000 patients and showed strong performance in predicting clinical outcomes, including melanoma relapse, pan-cancer prognosis and immunotherapy response predictions.

Fig. 1: Data curation, model development and evaluation.
figure 1
a, MUSK model pretraining. We developed a vision–language foundation model built upon a multimodal transformer architecture as the network backbone. Model pretraining consisted of two sequential phases. First, MUSK was pretrained on 50 million pathology images and one billion pathology-related text tokens. The images originated from nearly 33,000 whole-slide histopathology scans from 11,577 patients, representing 33 tumour types. Adapted from BEiT3 (ref. 21) architecture, the MUSK model consisted of shared self-attention blocks and two independent experts for vision and language inputs; pretraining was achieved using masked modelling. Second, MUSK was pretrained on one million image–text pairs from the model QUILT-1M using contrastive learning for multimodal alignment. b, General-purpose clinical application. Once the pretraining is complete, MUSK can be used for various downstream tasks with minimal or no further training. Importantly, we evaluated MUSK using whole-slide images and clinical reports for outcome prediction, including relapse, prognosis and immunotherapy response predictions. MUSK substantially improved upon state-of-the-art vision–language foundation models, including PLIP15, QUILT-1M46, BiomedCLIP47 and CONCH16. The graphics of reports, melanoma, prognosis, lung cancer and gastro-oesophageal cancer in b were created using BioRender (https://biorender.com). MIL, multiple instance learning.

Full size image

Zero-shot cross-modal retrieval
A key feature of a foundation model is its ability to perform downstream tasks without further training, that is, zero-shot learning capability15,16. By learning an aligned latent embedding space for visual and language representations, MUSK can retrieve relevant texts based on an image query and vice versa. We evaluated MUSK for zero-shot cross-modal retrieval on two benchmark datasets, BookSet22 and PathMMU23, with 4,265 and 7,774 image–text pairs, respectively.

MUSK achieved superior performance over the seven other foundation models in both image-to-text and text-to-image retrieval tasks (Fig. 2a and Supplementary Tables 1 and 2). On the PathMMU dataset, MUSK outperformed the second-best model (CONCH) with 34.4% (95% confidence interval (CI): 33.4–35.5%) versus 27.3% (95% CI: 26.4–28.3%) for image-to-text retrieval for Recall@50 (that is, recall rate of the top 50 retrieval candidates). Similarly, on the BookSet dataset, MUSK outperformed the second-best model (CONCH) with 74.8% (95% CI: 73.6–75.9%) versus 71.3% (95% CI: 70.0–72.6%) for Recall@50. We observed similar patterns for text-to-image retrieval tasks, with MUSK surpassing the second-best model with an improvement at Recall@50 of 4.0% and 7.5% in absolute terms. These results demonstrate the strong zero-shot learning capability of MUSK.

Fig. 2: Cross-modal retrieval and VQA.
figure 2
a, Zero-shot image-to-text and text-to-image retrieval. MUSK consistently outperformed the existing foundation models across different recall levels on BookSet and PathMMU. The two-sided Wilcoxon signed-rank test was used to assess the statistical differences between MUSK and the second-best model (CONCH). Visual examples are shown in Supplementary Fig. 4. b, VQA. MUSK substantially outperformed the existing foundation models in the PathVQA benchmark dataset. Notably, MUSK improved the accuracy by 7% over that of the best-performing model (K-PathVQA) specifically trained for VQA. Some examples of the results are shown for MUSK and PLIP models. The two-sided Mann–Whitney U-test was used to evaluate the statistical significance. For VQA task-specific models, no CIs were reported in the original papers. In a and b, data in foundation models are represented as the mean with 95% CIs estimated using the bootstrap method (n = 1,000 replicates).

Full size image

Visual question answering
In addition to cross-modal retrieval, another common vision–language task is VQA, which uses the input of pathology images and accompanying textual questions to generate an answer. Existing approaches require the design of sophisticated network models, specifically for this task24,25,26,27. By contrast, MUSK is a general-purpose vision–language foundation model that can perform VQA with minimal training (Fig. 1b and Supplementary Fig. 3). We evaluated the performance on the PathVQA28 dataset, which comprised 32,799 questions derived from 4,998 pathology images. MUSK achieved an accuracy of 73.2% (95% CI: 72.1–74.4%), significantly outperforming other vision–language foundation models, including PLIP15, QUILT-1M, BiomedCLIP and CONCH (Fig. 2b). Notably, MUSK surpassed the best-performing model, K-PathVQA27, specifically designed for VQA purposes (accuracy: 68.9%), highlighting the advantage of building powerful foundation models.

Image retrieval and classification
Although MUSK was developed as a multimodal foundation model, it can also serve as a stand-alone image encoder. Here, we demonstrate the vision capability of MUSK across various image-based tasks, including image retrieval and classification.

We evaluated the performance of zero-shot image retrieval on UniToPatho29 and BRACS30 datasets. In both datasets, MUSK outperformed the other foundation models across all evaluation metrics (Extended Data Fig. 1a,b and Supplementary Table 3). For example, MUSK exceeded CLIP by 22.3%, PLIP by 8.6% and CONCH by 2.5% in terms of mMV@5 (that is, the accuracy of the top 5 majority votes) on the BRACS dataset.

For image classification, we first evaluated the performance of the model for zero-shot learning on four benchmark datasets: PatchCamelyon31, SkinCancer32, PanNuke33 and UniToPatho29. Despite the challenging task of distinguishing multiple classes and the lack of any training data, MUSK still achieved promising performance for zero-shot image classification (Fig. 3a and Supplementary Table 4), surpassing the second-best model (CONCH, BiomedCLIP or QUILT-1M depending on the dataset) by 10.5%, 27.5%, 7.3%, and 10.1%.

Fig. 3: Patch-level image classification.
figure 3
a, Zero-shot image classification. MUSK consistently outperformed seven alternative foundation models when evaluated on the UniToPatho, SkinCancer, PatchCamelyon and PanNuke benchmark datasets, with P < 0.0001. b, Ten-shot image classification. MUSK consistently outperformed the other foundation models across the 12 benchmark datasets. Two-sided Wilcoxon signed-rank tests were used to calculate the statistical differences between MUSK and the top-performing alternative model. Data are presented as means and 95% CIs (error bars). These intervals were estimated using the bootstrap method (n = 1,000 replicates) (a) or calculated from n = 10 independent experiments (b).

Full size image

We then assessed the capability of MUSK for few-shot image classification, that is, using only a few samples to fine-tune the pretrained model. This can be useful when the amount of training data is small, either because it is practically difficult to annotate enough samples or because the prevalence of disease is low. We performed a comprehensive evaluation of the model for few-shot image classification using 12 benchmark datasets. These datasets contain expert-annotated histopathology images of normal samples and malignancies from various tissues/organs, such as skin, lung, colon, breast, prostate, kidney and lymph nodes.

Across all 12 datasets29,30,31,32,33,34,35,36,37,38,39, MUSK showed the highest accuracy for ten-shot image classification compared with the other foundation models (Fig. 3b, Extended Data Fig. 2a and Supplementary Table 5). Notably, the increase in classification accuracy was highest in the most challenging tasks, for which the other models did not perform well. For instance, in the UniToPatho dataset, MUSK achieved an increase of 9.8% over the second-best model. Furthermore, we evaluated the model for one-, two-, four- and eight-shot image classification with even fewer training samples and obtained similar results (Extended Data Fig. 1c). This indicates that MUSK is a robust and label-efficient vision encoder for image classification.

Finally, we evaluated the model for supervised image classification using all the available training data in each of the 12 benchmark datasets. MUSK achieved an average accuracy of 88.2%, outperforming other foundation models, including CLIP, PLIP, QUILT-1M, BiomedCLIP and CONCH by margins of 17.5%, 9.1%, 11.7%, 11% and 2.2%, respectively (Extended Data Fig. 2b and Supplementary Table 6). Visualizations of image embeddings produced by different models further highlight the robustness of the feature representation capabilities of MUSK (Supplementary Fig. 5). These results demonstrate that MUSK provides a powerful approach to learn more effective image representations for pathology classification.

Molecular biomarker prediction
Molecular biomarkers, such as protein expression and gene mutation, are critical components of precision oncology that can directly inform targeted therapies40. In this study, we evaluated the performance of MUSK against five state-of-the-art pathology foundation models for predicting molecular biomarkers from slide-level histopathology images. Specifically, we assessed the models for predicting receptor status in breast cancer and isocitrate dehydrogenase (IDH) mutation status in brain tumours using the publicly available Early Breast Cancer Core-Needle Biopsy (BCNB)41 and Medical University of Vienna (MUV-IDH)42 datasets, respectively. MUSK achieved significantly higher performance than other pathology foundation models, including PLIP15, UNI11, GigaPath10, Virchow12 and CONCH16, in predicting oestrogen receptor (ER), progesterone receptor (PR), human epidermal growth factor receptor 2 (HER2) status and IDH mutation status (Mann–Whitney U-test, P < 0.05; Extended Data Fig. 3a and Supplementary Table 7). For instance, our model achieved an area under the receiver operating characteristic curve (AUC) of 0.826 (95% CI: 0.813–0.839) for predicting HER2 status, which is a significant improvement over the leading comparison methods: GigaPath (0.786; 95% CI: 0.756–0.817) and CONCH (0.771; 95% CI: 0.745–0.796); P = 0.008.

Melanoma relapse prediction
Melanoma is the most serious form of skin cancer and has a relatively high likelihood of relapse that can lead to death. The accurate prediction of relapse after curative-intent surgery may inform personalized treatment strategies43. For instance, patients at high risk of relapse should receive adjuvant systemic therapy, whereas those at low risk of relapse may avoid the toxicity associated with the drugs. Traditional risk factors, such as tumour thickness and presence of ulceration, do not accurately predict an individual patient’s relapse44.

To address this unmet need, we developed a multimodal approach based on MUSK to predict the risk of relapse in melanoma. We used the VisioMel dataset45, which includes clinical reports and whole-slide images (WSIs) of diagnostic haematoxylin and eosin (H&E) slides, as well as 5-year follow-up data for 1,342 patients with melanoma. Compared with existing vision–language foundation models, MUSK achieved the highest AUC of 0.833 (95% CI: 0.818–0.847) for predicting 5-year relapse, significantly outperforming PLIP, QUILT-1M, BiomedCLIP and CONCH (Extended Data Fig. 4a,b). We then conducted ablation experiments using single-modal inputs on the MUSK model (Extended Data Fig. 4d). The results show that a model based on clinical reports or images alone has lower accuracy in predicting relapse. By combining the complementary information obtained from two different data modalities, MUSK further improved the accuracy of relapse prediction, highlighting the power of our multimodal approach.

To be clinically useful, a prognostic model should be highly sensitive in predicting relapse to minimize the risk of under-treatment. Thus, we evaluated the performance of the model at a predetermined sensitivity threshold of 90% (Extended Data Fig. 4c). The MUSK model achieved a substantially higher specificity than the other foundation models, with an improvement of about 12% (P = 0.0079). The clinical implication is that our model may spare more patients from toxic but unnecessary adjuvant therapy. Finally, visualization of the model’s prediction revealed that MUSK could automatically uncover relevant pathological features for predicting relapse (Extended Data Fig. 4e and Supplementary Fig. 6).

Pan-cancer prognosis prediction
Having demonstrated the effectiveness of our approach for predicting relapse, specifically in melanoma, we next evaluated the model for its ability to predict prognosis broadly in a pan-cancer setting. To do this, we collected diagnostic H&E WSIs, associated pathology reports and follow-up data from The Cancer Genome Atlas (TCGA), encompassing a total of 7,927 WSIs from 6,602 patients across 16 major cancer types. We trained a multimodal prognostic model for each cancer type and then evaluated its performance in predicting disease-specific survival.

Across all 16 cancer types, MUSK consistently outperformed the clinical risk factors and state-of-the-art foundation models for prognosis prediction. On average, MUSK achieved a concordance index (c-index) of 0.747, significantly above the c-index of 0.645 (P < 0.0001) for the overall stage; 0.668 (P < 0.0001), 0.672 (P < 0.0001), 0.668 (P < 0.0001) and 0.684 (P < 0.0001) for the multimodal foundation models PLIP15, QUILT-1M46, BiomedCLIP47 and CONCH16, respectively; and 0.681 (P < 0.0001), 0.681 (P < 0.0001) and 0.672 (P < 0.0001) for the pathology foundation models UNI11, GigaPath10 and Virchow12, respectively (two-sided Mann–Whitney U-test; Fig. 4a, Extended Data Fig. 3c, Supplementary Fig. 7 and Supplementary Table 7). The best prediction was achieved for renal cell carcinoma, with a c-index of 0.887 (95% CI: 0.854–0.920). In addition, MUSK achieved high performance for prognosis prediction in breast invasive carcinoma, colorectal adenocarcinoma, low-grade glioma and endometrial carcinoma, with a c-index above 0.8.

Fig. 4: Prognosis prediction across 16 cancer types.
figure 4
a, Kaplan–Meier analyses showed that MUSK can significantly stratify patients for disease-specific survival across 16 cancer types, with HRs ranging from 1.59 for glioblastoma multiforme to 36.83 for renal cell carcinoma. The two-sided log-rank test was used to compare the survival differences between the high-risk and low-risk groups (cut-off: median). b, The multimodal MUSK model significantly improved prognosis prediction over models based on clinical reports or WSIs alone, as shown in the overall bars (P < 0.0001). The overall bars represent the average performance across 16 projects. Bladder urothelial carcinoma (BLCA), breast invasive carcinoma (BRCA), cervical squamous cell carcinoma and endocervical adenocarcinoma (CESC), colorectal adenocarcinoma rectal adenocarcinoma (COADREAD), esophageal carcinoma (ESCA), glioblastoma multiforme (GBM), head and neck squamous cell carcinoma (HNSC), low-grade glioma (LGG), liver hepatocellular carcinoma (LIHC), lung adenocarcinoma (LUAD), lung squamous cell carcinoma (LUSC), pancreatic adenocarcinoma (PAAD), renal cell carcinoma (RCC), skin cutaneous melanoma (SKCM), stomach adenocarcinoma (STAD) and uterine corpus endometrial carcinoma (UCEC). In b, data are represented as the mean with standard deviation calculated using five-fold cross-validation experiments. The two-sided Mann–Whitney U-test was used to assess the statistical significance between MUSK and the comparison methods.

Full size image

We assessed the ability of the MUSK model to stratify patients for disease-specific survival using Kaplan–Meier analysis. Our results demonstrate a significant stratification of low-risk and high-risk patients with distinct survival outcomes (log-rank test, P < 0.001) across 16 cancer types (Fig. 4a). Strikingly, the model achieved a hazard ratio (HR) of greater than 30 in renal cell carcinoma, with a 10-year survival rate of 95.3% versus 48.3% in the low-risk and high-risk groups. We further conducted multivariable Cox regression analysis and confirmed that the MUSK-based risk score was a significant prognostic factor across all 16 cancer types, independent of clinical risk variables, including age, sex, stage and tumour grade (Supplementary Fig. 8).

We conducted ablation experiments on the MUSK model by training the image-only and text-only models for prognosis prediction. These models showed reasonable performance with average c-indices of 0.654 (P < 0.0001) and 0.673 (P < 0.0001), respectively. Of note, the multimodal MUSK model consistently outperformed the prognostic models with single-modal inputs across all 16 cancer types (Fig. 4b), with a significantly higher c-index of 0.746. These results demonstrate that MUSK can effectively integrate the complementary information of multimodal image and text data for prognosis prediction across cancer types.

Immunotherapy response prediction
Immunotherapy, specifically immune checkpoint inhibitors (ICIs), has transformed the treatment landscape in oncology and offers the potential for long-term durable benefits. However, only approximately 20% of patients respond to and benefit from ICIs48,49. It is critical to identify which patients will benefit from ICIs given the toxicity and financial burden of these treatments. Existing biomarkers, such as tumour programmed cell death ligand 1(PD-L1) expression and tumour mutation burden, have limited predictive power in distinguishing responders from non-responders50,51. There is an unmet need for a more accurate prediction of immunotherapy response.

Here, we collected multimodal datasets, including pretreatment H&E slides, associated pathology reports, therapy response and follow-up data for 118 patients with advanced non-small cell lung cancer (NSCLC) treated with ICIs. We evaluated the MUSK model for predicting two clinical endpoints: objective response and progression-free survival (PFS). The patients were classified as responders (complete or partial response) or non-responders (stable or progressive disease).

For predicting response, MUSK achieved an AUC of 0.768 (95% CI: 0.724–0.812), which was significantly higher than that of existing biomarkers, such as tumour PD-L1 expression, with an AUC of 0.606 (95% CI: 0.492–0.699; P < 0.0001). MUSK also outperformed the models trained using other multimodal foundation methods, such as PLIP, QUILT-1M, BiomedCLIP and CONCH, with AUC ranging from 0.636 to 0.692 (Fig. 5a). Similarly, for predicting PFS, MUSK demonstrated a significant improvement over existing biomarkers, with a c-index of 0.705 (95% CI: 0.677–0.732) compared with 0.574 (95% CI: 0.447–0.691; P < 0.0001) for tumour PD-L1 expression (Supplementary Fig. 10a). MUSK achieved a significantly higher performance for PFS prediction than the existing pathology foundation models, such as UNI, GigaPath and Virchow, with a c-index between 0.580 and 0.599 (Extended Data Fig. 3b and Supplementary Table 7). Compared with alternative multimodal approaches, MUSK also showed superior performance for PFS prediction over PLIP, QUILT-1M, BiomedCLIP and CONCH, with a c-index in the range of 0.601–0.640 (Fig. 5a).

Fig. 5: Lung cancer immunotherapy response prediction.
figure 5
a, MUSK substantially outperformed other foundation models in predicting objective response and PFS in patients with NSCLC treated with immunotherapy. b, The multimodal MUSK model significantly improved upon models based on clinical reports or WSIs alone for predicting immunotherapy response and outcomes. c, Kaplan–Meier analysis demonstrated that MUSK significantly stratified patients into high-risk and low-risk groups for PFS in the entire cohort and in clinically relevant subgroups defined by PD-L1 expression and epidermal growth factor receptor (EGFR) mutation status. The two-sided log-rank test was used to compare the survival differences between the high-risk and low-risk groups. d, Two examples of lung cancer cases with and without an objective response to immunotherapy. In each panel, the left image shows the original WSI, whereas the middle image displays the corresponding heat map that highlights the regions the model focused on within the WSIs. The right images provide enlarged views of the regions receiving the most attention from the model. The case with a response showed abundant infiltration of lymphocytes and minimal stroma. On the other hand, the case without a response showed minimal lymphocyte infiltration and abundant stroma. TPS, tumour proportion score. In a and b, the error bars represent the mean with standard deviation computed from five-fold cross-validation experiments, and the two-sided Mann–Whitney U-test was used to measure the statistical significance between MUSK and the compared methods.

Full size image

We compared the performance of the MUSK model with text-only and image-only models trained based on clinical reports and WSIs alone. MUSK significantly improved upon single-modal methods, demonstrating the effectiveness of our multimodal approach for predicting immunotherapy response and outcomes (Fig. 5b).

To assess the ability of MUSK to stratify patients for PFS, we performed Kaplan–Meier analysis (Fig. 5c). In the entire cohort, MUSK separated patients into two risk groups with HR of 2.54 (1.66–3.90); P < 0.0001. The median PFS were 4.3 and 13.4 months for the high- and low-risk groups, respectively. In comparison, tumour PD-L1 expression did not significantly stratify patients for PFS (Supplementary Fig. 10a). Our analysis revealed that MUSK can further stratify patients for PFS regardless of PD-L1 expression, EGFR mutation status and treatment regimens with single-agent ICI or chemo–ICI combination therapy (Fig. 5c and Extended Data Fig. 6a). The results are particularly striking for patients with PD-L1-negative (TPS = 0) tumours, with HR of 7.38 (2.15–25.38); P = 0.0002. These findings are clinically significant because patients with PD-L1-negative and EGFR-mutated tumours typically do not receive immunotherapy because of low response rates, but MUSK can identify a subset of these patients who may benefit from immunotherapy.

We further performed multivariate Cox regression analyses to evaluate the independent value of MUSK in predicting PFS. We incorporated all relevant clinical variables into the analysis, including age, sex, histology, central nervous system metastases, smoking, EGFR mutation and tumour PD-L1 expression. Our results indicate that MUSK is the most significant predictor of PFS with P = 0.0012 (Supplementary Fig. 9). Overall, these findings demonstrate that by integrating multimodal data, MUSK can provide valuable additional information regarding a patient’s likelihood of response to immunotherapy and, therefore, may potentially inform treatment decision-making.

To facilitate the interpretation of the model prediction, we generated attention heat maps and overlaid them on the WSIs (Fig. 5d). For patients predicted to have a high likelihood of response, the high-attention areas showed abundant infiltration of lymphocytes and minimal intratumoural stroma. On the other hand, for those with a low likelihood of response, the high-attention areas showed minimal intratumoural lymphocyte infiltration and abundant stroma with dense collagenous fibres. These findings suggest that the model could uncover pathological features previously implicated in immunotherapy response52.

Finally, we evaluated the multimodal MUSK for predicting response and outcome in 101 advanced gastro-oesophageal patients treated with ICI-based immunotherapy. The only established predictive biomarker in gastro-oesophageal cancer is microsatellite instability (MSI). In this cohort, MSI-H status had modest accuracy for predicting immunotherapy response, with an AUC of 0.616 (95% CI: 0.550–0.682; P < 0.0001). In comparison, MUSK achieved a much higher AUC of 0.762 (95% CI: 0.718–0.805), which significantly outperformed other multimodal foundation models, such as PLIP, QUILT-1M, BiomedCLIP and CONCH, with AUC between 0.652 and 0.693 (Extended Data Fig. 5a). MUSK was also superior to pathology-based foundation models, including UNI, GigaPath and Virchow, with AUC ranging from 0.644 to 0.651. Similar results were obtained for predicting PFS, with MUSK outperforming the other foundation models (Extended Data Fig. 5a). Consistent with the results for lung cancer, the multimodal MUSK model significantly improved upon text-only and image-only models for predicting immunotherapy response and outcomes in gastro-oesophageal cancer (Extended Data Fig. 5b).

We performed Kaplan–Meier analysis to assess MUSK for stratifying patient outcomes (Extended Data Fig. 5c). Whereas PD-L1 expression did not significantly stratify patients for PFS (Supplementary Fig. 10b), MUSK separated patients into two risk groups with HR of 3.49 (2.02–6.01); P < 0.0001. The median PFS were 3.6 and 14.1 months for the high- and low-risk groups, respectively. MUSK further stratified patients within biomarker-defined subgroups, such as PD-L1-negative (combined positive score = 0) and PD-L1-positive (combined positive score ≥ 1) tumours as well as microsatellite stable/MSI-L tumours. In addition, MUSK stratified patients regardless of treatment regimens with either single-agent ICI or chemo–ICI combination therapy (Extended Data Fig. 6b). Finally, we performed multivariate Cox regression analyses, and our results showed that MUSK was the only significant predictor of PFS (P = 0.0013), in addition to MSI status (Extended Data Fig. 5d). Visualization of attention heat maps revealed differential patterns of lymphocyte infiltration and stroma abundance in responders versus non-responders (Extended Data Fig. 5e).

Discussion
In this study, we present MUSK, a new vision–language foundation model for general-purpose oncology applications. Through extensive benchmark evaluation of 23 downstream tasks, we show that MUSK achieves superior performance over existing foundation models, with minimal or no further training, for applications in cancer detection, diagnosis and grading. Importantly, in contrast to previous studies that relied on the similarity between different data modalities, we leveraged the complementary information between clinical reports and images and demonstrated that the multimodal approach achieved superior outcome prediction over either modality alone. Specifically, MUSK showed strong performance in melanoma relapse prediction, prognosis prediction across 16 cancer types and immunotherapy response prediction in two real-world cohorts of lung and gastro-oesophageal cancers.

The performance gain achieved by MUSK is largely attributable to its ability to incorporate unpaired image and text data for pretraining, which is far more common than paired data. Existing studies have used off-the-shelf foundation models with contrastive learning, which requires paired image–text data for pretraining. By contrast, MUSK is a custom-designed foundation model pretrained with unified masked modelling. This allowed us to leverage substantially larger and more diverse unpaired data (50 million images and one billion text tokens), which represent an increase of several orders of magnitude over approximately one million image–text pairs used in previous studies15,16.

The scarcity of complete multimodal data represents a major challenge for training reliable AI models4. Our approach provides an effective solution to this problem by using more readily available unimodal data for unified masked learning, followed by using multimodal data for fine-tuning and alignment. This training paradigm can be extended and applied in building multimodal foundation AI models in other domains beyond pathology, such as radiology and dermatology images/reports7, as well as structured data, such as genomics53.

Accurate prediction of treatment response and outcomes has significant clinical implications for precision oncology20. There are important conceptual and practical distinctions between cancer detection and diagnosis, the primary focus of existing pathology foundation models10,11,12,16. Given that pathologists have excellent performance in diagnosing cancer (which is the current gold standard), the impact of AI models in such scenarios would be limited to an assistive role. However, outcome prediction is a much more challenging problem because of the inherent uncertainty associated with forecasting the future. Current approaches based on clinical risk factors, such as cancer stage and tumour grade, have limited accuracy, typically with a c-index of approximately 0.65, leaving room for improvement. By combining routine clinical reports and histopathology images, the multimodal MUSK model significantly improved upon traditional risk factors for prognosis prediction across 16 cancer types, with an average c-index of 0.75 and exceeding 0.8 for several cancers. This model may be used to complement the current staging system and refine risk stratification, paving the way for personalized treatment strategies. For instance, in early-stage cancers, adjuvant therapy can be given to patients at a high risk of relapse after curative-intent surgery, whereas low-risk patients may avoid the toxicity associated with systemic drugs.

Immunotherapy, particularly ICIs, has prolonged the survival of many patients with metastatic cancers and is the standard of care for the treatment of most tumour types. However, only a small fraction (10–20%) of patients respond to immunotherapy and experience durable clinical benefits54. Given the financial burden and potential for immune-related toxicities of these treatments55, it is crucial to identify patients who are most likely to respond to and benefit from ICIs. Here, we fine-tuned our pretrained multimodal foundation model for predicting immunotherapy response based on routine clinical reports and histopathology images. The model significantly improved upon existing clinical biomarkers, such as PD-L1 expression and MSI status in lung and gastro-oesophageal cancers, which are among the most common and lethal cancers worldwide56. To aid in model interpretation, we applied visualization techniques based on attention heat maps, which revealed pathological features of the tumour microenvironment that were consistent with known mechanisms of response and resistance to immunotherapy52,57. Importantly, the model identified a subset of patients with PD-L1-negative or EGFR-mutated tumours who could benefit from ICIs. Because these patients typically do not receive ICIs because of their overall low response rates54,58, our findings have significant clinical implications with the potential for broadening the population of patients who may benefit from ICIs.

Although the results of immunotherapy response prediction are promising, it is worth noting that they were obtained based on relatively small cohorts of about 220 patients from one academic medical centre. Before the model can be considered for clinical implementation and adoption, several steps are needed to ensure that it is rigorously evaluated for safety, efficacy and clinical utility. First, these findings should be validated and confirmed in future studies with larger, multi-institution cohorts. Second, for high-stakes applications, such as treatment decision-making, regulatory approval is required, including validation in prospective clinical trials of immunotherapy-treated patients from diverse populations. The generation of high-level evidence through rigorous prospective validation can ultimately lead to changes in the clinical guidelines and clinical practice.

In conclusion, we propose a new vision–language foundation model by leveraging unpaired image–text data. The model provides an effective approach to the integration of pathology images and clinical reports and can potentially improve diagnosis and precision cancer therapy.

Methods
Model design and pretraining
The pretraining of MUSK, inspired by BEiT3 (ref. 21), consisted of two main steps. The first step used masked data modelling to leverage large-scale unpaired images and text. The second step utilized around one million image–text pairs with contrastive learning to align the two modalities and establish connections between images and texts. The network backbone is a general-purpose multimodal transformer inspired by mixture-of-experts networks in large language models59, multimodal pretraining21,60 and image generation61. The model configurations are specified in Extended Data Fig. 7 and Supplementary Table 16.

Multimodal data curation for pretraining
For pretraining of the multimodal MUSK foundation model, we incorporated unpaired pathology images and texts for masked learning, as well as paired image–text data for contrastive learning. The masked pretraining dataset comprised one billion text tokens extracted from 1,001,800 pathology-related articles from the PubMed Central Open Access Dataset and 50 million pathology image patches from TCGA. The image patches were obtained from nearly 33,000 digitized H&E-stained WSIs from 11,577 patients representing 33 tumour types. We used the QUILT-1M46 dataset (802,000 image–text pairs) in addition to the PathAsst62 dataset (207,000 image–text pairs) for the second pretraining phase via contrastive learning.

Noisy image–text pairs collected from the web present challenges for training and may degrade the model performance. Thus, instead of training directly on these datasets, we adopted a bootstrap approach similar to BLIP63 during contrastive learning. We initially trained on QUILT-1M to obtain a baseline model and then filtered out low-similarity image–text pairs based on this model. The final model was trained on the refined image–text dataset with an improved data quality (Supplementary Fig. 2).

Unified masked pretraining
We used a unified masked data modelling approach for pretraining. We sampled a batch of training images and texts during training to apply masked loss and optimize the model. We utilized masked language modelling (MLM) loss for texts and masked image modelling (MIM) loss for images.

Masked language modelling
Similar to bidirectional encoder representations from transformers (BERT)64, we randomly selected 15% of the tokens within a text sequence and replaced them with special [MASK] tokens. The model was then trained to predict these masked tokens using the context provided by all other unmasked text tokens. The positions of the masked tokens are denoted as , where  is the total number of input tokens. The input sequence with the masked tokens is . The output vectors , corresponding to the masked token positions, are fed into a classifier. The classifier predicts the most probable words from the vocabulary for each masked position using cross-entropy loss as the objective function:


(1)
Masked image modelling
The input image  was split into  image patches  and then tokenized into  as the output labels of MIM using an image tokenizer. The vocabulary  contained discrete token indices. At the input layer, image patches were randomly masked, and then the model predicted the visual tokens  of the masked patches. The masked positions are denoted as . Next, we replaced the masked patches with a learnable embedding , making the input corrupted image patches  that are fed into the transformer. The pretraining objective was to maximize the log-likelihood of the correct visual tokens  given the corrupted image:


(2)
An image tokenizer is required to obtain semantically meaningful visual tokens. However, existing tokenizers, such as DALL-E65 and BEiT-v2 (ref. 66), are primarily trained on natural images. Because the image tokenizer defines the learning targets for MIM, using a non-pathology-specific tokenizer could result in suboptimal image representations. To address this, we trained a pathology-specific image tokenizer for MUSK following the BEiT-v2 methodology66, utilizing five million pathology images from the TCGA dataset. For training, we adopted CTransPath67 as the teacher model, providing semantic-aware targets to enhance the tokenizer performance.

Masked training settings
Image augmentations included random vertical flip (P = 0.5), colour dropping (P = 0.2) to convert images to greyscale and weak colour jittering (P = 0.8) with specific adjustments to brightness, contrast, saturation and hue. Additionally, RandStainNA68 and multiple fields of view (FoVs)69, which involve random magnifications at ×10, ×20 and ×40, were incorporated into the training pipeline. We pretrained MUSK for one million steps using the masked pretraining loss of LMIM for images and LMLM for texts. The batch sizes were 2,048 for images and 2,048 for texts. MUSK used an input image with 384 × 384 pixels and then patched as 16 × 16 pixels. Text data were tokenized using the SentencePiece tokenizer with a vocabulary size of 64,000. We used the AdamW70 optimizer with β1 = 0.9, β2 = 0.95 and ϵ = 1 × 10−8 for optimization. We used a cosine learning rate decay scheduler with a peak learning rate of 1.5 × 10−3 and a linear warmup of 10,000 steps. The weight decay was set as 0.05, and the stochastic depth with a rate of 0.1 was used.

Contrastive pretraining
The second pretraining step utilized contrastive learning to further train MUSK on image–text pairs for modality alignment. Image embeddings and text embeddings were used to compute the contrastive loss 18. Contrastive loss aims to align the global representation of images and texts. We further designed an auxiliary loss for fine-grained modality alignment. Specifically, we constructed a lightweight cross-attention decoder module utilizing images as side information for MLM. Image embeddings were used as the key and value in cross-attention, whereas language embeddings served as queries. This approach encourages language embedding to explore more detailed interactions with images, ultimately enhancing modality alignment. We empirically masked 30% of the input text tokens and predicted ground-truth labels. We built a prediction layer at the output hidden states of the cross-modal decoder and finally optimized the model through cross-entropy loss . The training loss for modality alignment is a combination of contrastive loss and auxiliary MLM loss with the decoder (Extended Data Fig. 7b):


(3)
Contrastive training settings
We pretrained MUSK with contrastive learning using the loss function  for 20 epochs. The batch size was 3,072 image–text samples. MUSK used an input image with 384 × 384 pixels and then patched as 16 × 16 pixels. We applied image augmentations, such as random resized cropping, horizontal flipping and colour jittering, to enhance the training data. Text data were tokenized using a SentencePiece tokenizer with a vocabulary size of 64,000. We used the AdamW optimizer with β1 = 0.9, β2 = 0.95 and ϵ = 1 × 10−8 for optimization. We used a cosine learning rate decay scheduler with a peak learning rate of 1 × 10−4 and a linear warm-up of two epochs. The weight decay was set to 0.05, and the stochastic depth rate was 0.1.

Ablation study
MUSK enhances traditional masked pretraining and contrastive learning by introducing four key adaptations: pathology-specific augmentations (stain augmentations and multiple fields of view), a pathology-specific tokenizer for MIM, a fine-grained image–text decoder for better local alignment and bootstrapped contrastive learning to filter noisy data. We performed a series of ablation studies that demonstrated that these adaptations are essential for optimizing model performance and significantly improving image representation, cross-modal learning and data quality for precision oncology applications (Extended Data Fig. 8 and Supplementary Results).

Benchmark datasets
We evaluated the MUSK model for multimodal retrieval, VQA and histopathology image classification using various publicly available benchmark datasets. BookSet22 contains 4,265 image–text pairs for cross-modal retrieval, whereas PathMMU23 includes 7,774 annotated image–caption pairs, emphasizing retrieval with expert-reviewed cases. PathVQA28 provides 32,799 open-ended questions linked to 4,998 pathology images for VQA. For histopathology classification, PatchCamelyon31 contains 327,680 binary-labelled images, and NCT-CRC-HE-100K37 spans 107,180 images across nine colorectal tissue classes. SICAPv234 focuses on prostate histology with 12,081 patches in four classes, whereas Osteo38 contains 1,144 osteosarcoma images across three classes. RenalCell36 features 35,458 images for renal carcinoma classification with six tissue types, and SkinCancer32 includes 129,364 patches representing 16 skin conditions. LC2500035 contains 25,000 images split into lung and colon cancer subsets, with five classes in total. PanNuke33 (https://www.drivendata.org/competitions/148/visiomel-melanoma/page/673/) includes over 200,000 nuclei annotations across 19 tissue types for binary classification. UniToPatho29 supports colorectal polyp grading with 9,536 patches in six classes, and WSSS4LUAD39 targets LUAD tissue classification with 10,091 images in three categories. BRACS30 offers 4,539 breast image patches with six lesion types, whereas BCNB41 and MUV-IDH42 include 1,057 and 872 slides, respectively, with multimodal clinical information for breast cancer and brain tumour classification. Detailed descriptions of each dataset are provided in Supplementary Methods.

Melanoma relapse prediction
We evaluated the MUSK model for predicting risk of relapse in melanoma by combining information from histopathology images and clinical reports. For this purpose, we used the VisioMel Challenge dataset45, curated from the French national database on melanoma. This dataset contained clinical reports, diagnostic H&E WSIs and follow-up data for 1,342 patients with melanoma. For comparison, we trained three models based on MUSK: (1) an image-based model, (2) a text-based model and (3) a multimodal classifier that integrates image and text data. To evaluate these models, we conducted five-fold cross-validation through stratified sampling based on relapse.

Because MUSK-V (the vision part of MUSK) is a patch-level encoder, we performed patch aggregation using attention-based multiple instance learning (AbMIL)71. It consists of a fully connected layer and rectified linear unit nonlinearity that maps the inputs to an embedding dimension of 512. This stage followed a two-layer gated attention network with a hidden dimension of 384. The network uses a fully connected classifier head that projects attention-pooled slide-level image embeddings onto the desired dimension. Meanwhile, clinical reports—providing details such as the patient’s age at initial diagnosis, sex, primary site and medical history—were encoded using MUSK-L, the language component of MUSK. Finally, we combined the slide-level image embeddings and text embeddings as input to a lightweight multilayer perceptron classifier, which generated the prediction outputs.

We trained each model for 100 epochs on the training set using an AdamW optimizer and a cosine learning rate scheduler with an initial learning rate of 0.001. We used a weighted data sampler that balanced the sampling probability of each outcome label during each epoch. We set up early stopping criteria if the evaluation metric did not improve for ten consecutive epochs. We used dropout at 0.25 after intermediate layers in the network for regularization.

Pan-cancer prognosis prediction
We evaluated the MUSK model for predicting survival outcomes across various cancer types by combining histopathology images and clinical reports. For this purpose, we used 7,927 whole-slide diagnostic H&E images from 6,602 patients across 16 cancers (BLCA, BRCA, CECS, COADREAD, ESCA, GBM, HNSC, LGG, LIHC, LUAD, LUSC, PAAD, RCC, SKCM, STAD and UCEC) in TCGA72. These datasets were selected based on the size of the cohort and ratio of uncensored to censored patients. We excluded cancer types with fewer than 100 patients or fewer than 5% of the patients who had an outcome event. Each diagnostic H&E slide was associated with a detailed pathology report73. The clinical outcome endpoint was disease-specific survival. We trained a prognostic model separately for each cancer type and evaluated its performance using five-fold cross-validation by stratified sampling based on survival status. We compared the multimodal MUSK model with the unimodal approach, which only utilizes histopathology images or clinical reports as input.

To obtain a slide-level prediction, we used AbMIL to aggregate the image features. Preprocessing is necessary for clinical reports because their text length exceeds the 100-token limit for MUSK-L. Here, we leveraged a large language model (GPT-4 (ref. 74)) to generate structured reports with more succinct and informative summaries based on full clinical reports. This was achieved by applying expert-designed prompts, as shown in Supplementary Table 8. This procedure allowed us to capture the most relevant information pertinent to survival outcomes, such as patient characteristics, tumour size, differentiation, invasion and lymph node metastases. The training details were the same as those for melanoma relapse prediction.

Immunotherapy response prediction
We evaluated the MUSK model for predicting immunotherapy response and outcomes in two real-world NSCLC and gastro-oesophageal cancer cohorts. Both cohorts were obtained from the Stanford University Medical Center, with approval from the institutional review board. The requirement for informed consent was waived for this retrospective analysis. The inclusion criteria were advanced (metastatic or recurrent) NSCLC or gastro-oesophageal cancer (originating from the oesophagus, gastro-oesophageal junction or stomach) treated with anti-PD1 or anti-PD-L1 immune checkpoint blockade between 2018 and 2023, with an available H&E-stained tissue section from a pretreatment core needle or surgical tumour biopsy. Patients were identified using the Stanford Research Repository75. In both cohorts, patients were treated with various lines of immunotherapy with or without concurrent chemotherapy. The best overall response was evaluated through manual curation of radiology reports, and the patients were divided into two groups: responders (complete or partial response) and non-responders (stable or progressive disease). PFS was determined from the date of treatment initiation until the date of progression or death. Patients who did not progress were censored at the date of the last follow-up.

Our study included 118 patients with NSCLC and 101 patients with gastro-oesophageal cancer treated with immunotherapy (Supplementary Tables 11 and 12). We trained multimodal MUSK models using the diagnostic H&E WSIs and associated pathology report to predict the objective response and PFS. We used the AbMIL method to aggregate histopathology image features and obtain a slide-level prediction. We used GPT-4 to standardize clinical reports by applying expert-designed prompts, as shown in Supplementary Table 9 for lung cancer and Supplementary Table 10 for gastro-oesophageal cancer. We concatenated image and report embeddings, which constituted a multimodal embedding for predicting immunotherapy response and outcomes. The models were evaluated through five-fold cross-validation with stratified sampling of relevant outcomes for each cohort. The training details were the same as those for melanoma relapse prediction.

Model visualization
To enhance the model interpretability, we generated heat maps76 to display the regions related to the model predictions on the WSIs. We cropped WSIs into tiles with 85% overlap and calculated the attention scores for each tile to provide more detailed heat maps. These attention scores were normalized to a 0–1 scale, with warmer colours on the heat map indicating higher attention scores, thus highlighting areas more relevant to the predictions of the model. The heat maps were superimposed onto the original WSIs with a semi-transparent overlay.

Statistical analysis
For zero-shot or fine-tuned tasks with independent test sets, we evaluated performance variations using a non-parametric bootstrapping method, deriving 95% CIs from 1,000 bootstrap samples. For five-fold cross-validation tasks, 95% CIs were estimated based on the results from the five folds. The two-sided Mann–Whitney U-test or two-sided Wilcoxon signed-rank test (as indicated in the figure captions) was used to assess statistical significance. We used AUC to evaluate the performance of melanoma relapse prediction and immunotherapy response prediction. We used the c-index to evaluate the performance of the prognostic models for survival endpoints. Kaplan–Meier curves were generated to assess patient stratification, using the median predicted risk score as the cut-off value. The statistical significance of low-risk and high-risk patient groups was assessed using the log-rank test.

Inclusion and Ethics Statement
The study was approved by the Institutional Review Board of Stanford University (protocol #67432). The requirement for informed consent was waived for this retrospective analysis.

Reporting summary
Further information on the research design is available in the Nature Portfolio Reporting Summary linked to this article.

Data availability
The histopathology and clinical data for TCGA used in this study are publicly available through the following resources: National Cancer Institute Genomic Data Commons Portal (https://portal.gdc.cancer.gov/), cBioPortal (https://www.cbioportal.org/) and TCGA Pathology Reports (https://github.com/tatonetti-lab/tcga-path-reports). Additional datasets used include QUILT-1M (https://github.com/wisdomikezogwo/quilt1m), PathAsst (https://huggingface.co/datasets/jamessyx/PathCap), PathVQA (https://huggingface.co/datasets/flaviagiammarino/path-vqa), BookSet and PubmedSet (https://warwick.ac.uk/fac/cross_fac/tia/data/arch), PatchCamelyon (https://patchcamelyon.grand-challenge.org/), NCT-CRC-HE-100K (https://zenodo.org/record/1214456), SICAPv2 (https://data.mendeley.com/datasets/9xxm58dvs3/1), Osteo (https://www.cancerimagingarchive.net/collection/osteosarcoma-tumor-assessment/), RenalCell (https://zenodo.org/records/6528599), SkinCancer (https://www.isic-archive.com/), LC25000 (https://github.com/tampapath/lung_colon_image_set), PanNuke (https://warwick.ac.uk/fac/cross_fac/tia/data/pannuke), UniToPatho (https://ieee-dataport.org/open-access/unitopatho), WSSS4LUAD (https://wsss4luad.grand-challenge.org/WSSS4LUAD/), BRACS (https://www.bracs.icar.cnr.it/), Visiomel (https://www.drivendata.org/competitions/148/visiomel-melanoma/), PathMMU (https://huggingface.co/datasets/jamessyx/PathMMU), BCNB (https://bcnb.grand-challenge.org/) and MUV-IDH (https://doi.org/10.25493/WQ48-ZGX). The data for patients in the immunotherapy cohorts are subject to controlled access because they contain sensitive information about patient privacy. Requests for access may be submitted to the corresponding author (rli2@stanford.edu) with a brief research plan and require consent to data use agreement. We aim to respond to all data access requests within 4 weeks. Data usage is restricted to non-commercial academic research purposes.

Code availability
Our work is publicly available at https://github.com/lilab-stanford/MUSK, including installation instructions, model weights, evaluation code, and example data.