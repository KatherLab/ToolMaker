MedS3: Towards Medical Small Language Models with Self-Evolved Slow Thinking

Shuyang Jiang,♣,♢, Yusheng Liao∗,♠,♢, Zhe Chen♠,♢, Ya Zhang♠,♢, Yanfeng Wang♠,♢, Yu Wang,♠,♢
♠Shanghai Jiao Tong University
♣Fudan University
♢Shanghai Artificial Intelligence Laboratory
shuyangjiang23@m.fudan.edu.cn
{liao20160907,chenzhe2018,wangyanfeng622,ya_zhang,yuwangsjtu}@sjtu.edu.cn
Equal contribution.Corresponding Author
Abstract

Medical language models (MLMs) have become pivotal in advancing medical natural language processing. However, prior models that rely on pre-training or supervised fine-tuning often exhibit low data efficiency and limited practicality in real-world clinical applications. While OpenAI’s O1 highlights test-time scaling in mathematics, attempts to replicate this approach in medicine typically distill responses from GPT-series models to open-source models, focusing primarily on multiple-choice tasks. This strategy, though straightforward, neglects critical concerns like data privacy and realistic deployment in clinical settings. In this work, we present a deployable, small-scale medical language model, MedS3, designed for long-chain reasoning in clinical tasks using a self-evolution paradigm. Starting with a seed dataset of around 8,000 instances spanning five domains and 16 datasets, we prompt a base policy model to perform Monte Carlo Tree Search (MCTS) to construct verifiable reasoning chains. Each reasoning step is assigned an evolution rollout value, allowing verified trajectories to train the policy model and the reward model. During inference, the policy model generates multiple responses, and the reward model selects the one with the highest reward score. Experiments on eleven evaluation datasets demonstrate that MedS3 outperforms prior open-source models by 2 points, with the addition of the reward model further boosting performance (
∼
13 points), surpassing GPT-4o-mini. Code and data are available at https://github.com/pixas/MedSSS.

MedS3: Towards Medical Small Language Models with Self-Evolved
Slow Thinking


Shuyang Jiang†,♣,♢, Yusheng Liao∗,♠,♢, Zhe Chen♠,♢, Ya Zhang♠,♢, Yanfeng Wang♠,♢, Yu Wang†,♠,♢	♠Shanghai Jiao Tong University	♣Fudan University	♢Shanghai Artificial Intelligence Laboratory	shuyangjiang23@m.fudan.edu.cn	{liao20160907,chenzhe2018,wangyanfeng622,ya_zhang,yuwangsjtu}@sjtu.edu.cn


1 Introduction

Models	Without	Without	Step-wise	Small Size	Slow Thinking	Process	Pretraining	Close-sourced Teacher	Search Support	Reward Usage	MMed-Llama3	✗	✗	✗	✓	✗	✗	UltraMedical	✓	✗	✗	✓	✗	✗	HuatuoGPT-o1	✓	✗	✗	✓	✓	✗	O1-journey Part 3	✓	✗	✗	✗	✓	✗	MedS3 	✓	✓	✓	✓	✓	✓ 
Table 1: Comparison of MedS3 with other medical models. Our MedS3 supports flexible inference-time scaling on resource-constrained devices, as well as process reward-guided decoding algorithms. Furthermore, MedS3 is a self-evolved model without dependence on large proprietary models for distillation or critique.
Large Language Models (LLMs) have demonstrated significant potential in the medical domain (Singhal et al., 2023; Nori et al., 2023; Chen et al., 2023b), with applications ranging from generating clinical notes(Biswas and Talukdar, 2024; Jung et al., 2024) to supporting patient communication (Tu et al., 2024; Liao et al., 2024b). Recently, slow-thinking reasoning models, exemplified by OpenAI o1, have shown impressive improvements on reasoning-intensive mathematical problems(OpenAI, 2024; Wang et al., 2024). However, limited efforts have been made to induce similarly strong reasoning abilities in medical-oriented language models.

Before the rise of OpenAI o1, researchers focused on improving medical language models (MLMs) primarily through extensive pre-training, which demands large computational resources and often yields only modest gains in downstream tasks. For instance, MMed-Llama3 post-pretrained Llama3 8B on 25.5B tokens but achieved only about a five-point performance improvement compared to its base model (Qiu et al., 2024). Other works (Christophe et al., 2024; Ankit Pal, 2024) using supervised fine-tuning(Ouyang et al., 2022) are more computationally efficient but impart limited knowledge to the model, restricting performance gains on tasks outside the fine-tuning corpus. Moreover, many human-annotated medical datasets provide only concise responses (either short phrases or a single ground truth option), and fine-tuning on such data can degrade an MLM’s broader language fluency, reducing its practical value in realistic clinical scenarios. On the other hand, synthetic corpora generated by LLMs (Luo et al., 2024; Qiu et al., 2024) often contain hallucinations (Xu et al., 2024b; Huang et al., 2023), so directly using these outputs for teacher-forcing can limit a target model’s optimization space. Consequently, focusing on inference-time scaling, often referred to as “slow thinking,” emerges as a data-efficient approach with the potential to enhance general performance while mitigating the drawbacks of pre-training- or fine-tuning-heavy strategies.

A closely related work to slow thinking is HuatuoGPT-o1 (Chen et al., 2024), which employs GPT-4o to generate reasoning-intensive problems and corresponding complex reasoning steps for distillation and reinforcement learning. While it achieves certain levels of long-chain reasoning, its heavy reliance on large proprietary models and the inherent instability of Proximal Policy Optimization (PPO) training (Dulac-Arnold et al., 2021) limits its generalizability to other clinical applications.

Another relevant work, O1 Journey Part 3 (Huang et al., 2025), directly distills OpenAI o1’s outputs into 
∼
70B-parameter models without leveraging a fine-grained process reward model (PRM). This approach compromises user-friendliness and data privacy, and it also struggles to detect hallucinations. Moreover, relying exclusively on multiple-choice problems for distillation constrains its applicability to a broader range of clinical tasks.

In contrast, this work introduces MedS3, a small-scale medical language model equipped with robust long-chain reasoning capabilities (the “policy”) and an accompanying fine-grained process reward model (the “PRM”). Our approach uses a self-bootstrapping pipeline to iteratively enhance the model’s performance across diverse clinical tasks. Specifically, we curate 16 medical tasks from established medical training corpora, encompassing clinical diagnosis QA, medical natural language inference, knowledge-intensive QA, long-context QA, and biomedical QA. We then sample 500 seed instances from each task to form an initial dataset of approximately 8,000 instances, which kickstarts our self-evolution process. Leveraging a Monte-Carlo Tree Search (MCTS)-based evolution pipeline, we iteratively generate synthetic datasets for both policy fine-tuning and PRM training. Verified synthetic trajectories are preserved to refine the policy model, while intermediate reasoning steps are labeled with MCTS rollout values to build the PRM dataset. By fine-tuning the base model on this enriched policy dataset and further enhancing it using the PRM, our method achieves state-of-the-art performance across eleven clinical benchmarks, surpassing GPT-4o-mini in tasks that demand precise verification of each reasoning step.

The uniqueness of MedS3 is further underscored by a comprehensive comparison in Table 1, which demonstrates its superiority over other medical models in robust long-chain reasoning and breadth of clinical-task coverage.

Overall, we summarize our contributions as follows:

1. First Self-Evolution Framework: To the best of our knowledge, this work introduces the first self-evolution framework specifically designed to empower small-scale medical models with long-chain reasoning capabilities, enabling efficient performance across a wide range of clinical applications.
2. State-of-the-Art Performance: Our self-evolved system, MedS3, achieves state-of-the-art performance across eleven clinical benchmarks, outperforming all open-source models and GPT-4o-mini. This is driven by the integration of a fine-grained PRM that enhances reasoning accuracy at each step.
3. Open-Source Resources for Research: We openly release both the policy fine-tuning corpus and the process reward model corpus, providing valuable resources for future research and fostering further advancements in medical AI.
2 MedS3

Framework This section presents a detailed overview of the proposed MedS3 framework, which is structured into four key components:

1. Self-Bootstrapping Evolution which synthesizes reasoning trajectories for instances of the training set with Monte-Carlo Tree Search (MCTS) technique using the base policy 
π
0
.
2. Policy Model 
π
 which is derived by fine-tuning the base policy 
π
0
 using the generated synthetic data in a supervised learning manner.
3. Process Reward Model (PRM) 
V
θ
 which is fine-tuned with step-wise supervision and assigns a float value in the range
[
0
,
1
]
 to each reasoning step, representing the probability of step-wise correctness.
4. PRM-guided Inference which utilizes the MCTS algorithm to generate diverse solutions and determines the final solution by evaluating them with the PRM.
The overall framework is presented in Fig. 1.

Refer to caption
Figure 1: Overview of MedS3 framework. MedS3 utilizes a Monte-Carlo Tree Search pipeline to self-generate step-by-step reasoning paths for each question in the seed dataset (a). During this process, MedS3 uses result simulation to obtain the rollout value for each node (b); After obtaining the child’s rollout value, MedS3 executes back-propagate to enable precise value prediction from deeper layer to transfer back to shallow nodes (c). After gathering all correct and wrong finish nodes, we use supervised fine-tuning to optimize the policy model 
π
 with correct reasoning trajectories and step-wise discriminative loss to obtain a process reward model 
V
θ
 (d).
2.1 MCTS-guided Evolution

This algorithm builds upon an 
n
-ary tree, where each tree node 
T
 contains the following attributes: (1) reasoning step 
s
, which is an intermediate step tracing from the root; (2) value 
v
, which is an evaluation of current contribution to the correctness of the whole reasoning trace; (3) the children nodes 
{
c
}
, which is a collection of nodes that continue reasoning from the current node; (4) the parent node 
p
 which is the former reasoning step and (5) the number of visits 
n
. Every root node is initialized as 
T
=
(
[
s
0
]
,
0
,
∅
,
null
,
0
)
 where 
s
0
=
“Let s break down this problem step by step.” to guarantee a multi-step reasoning process. There are five stages in a full MCTS pipeline, including Node Selection, Node Expansion, Node Rollout, and Backpropagation.

Node Selection

This process starts from an initial root to select the next node. Within each iteration, we use UCB (Winands et al., 2008) as the criterion to select a child, which is as follows:

U
⁢
C
⁢
B
⁢
(
T
)
=
v
C
+
c
⁢
ln
⁡
n
T
p
⁢
a
⁢
r
⁢
e
⁢
n
⁢
t
n
T
(1)
where 
T
p
⁢
a
⁢
r
⁢
e
⁢
n
⁢
t
 is the parent of current node 
T
 and 
c
 is an exploration constant, which is set to 2. For each intermediate node, we select its child node with the highest 
U
⁢
C
⁢
B
 value. We choose this criterion to expect models to further explore those nodes with high rollout values but low visiting counts, which inhibits models from repeatedly expanding already high-value nodes and encourages the expansion of underexplored nodes with the second highest value.

Node Expansion

After picking up the candidate node 
T
c
 using the UCB criterion, we expand the reasoning steps of the current node. If the current node possesses a relatively high value (
v
c
≥
t
⁢
h
⁢
r
, where 
t
⁢
h
⁢
r
=
0.9
 is a pre-defined threshold), we prompt the node to directly generate a Finish node to accomplish this path reasoning. This manual operation not only does not impact the reasoning correctness as a value is close to 1 only when the trace 
[
s
0
,
s
1
,
⋯
,
s
k
]
 is close to the correct final answer, but also reduces unnecessary exploration tokens. Otherwise, assume that the selected node is located at 
k
-th depth among the tree with previous reasoning trajectories 
[
s
0
,
s
1
,
⋯
,
s
k
]
 connected by a coherence phrase 
t
s
, we sample 
B
 single-step outputs 
{
s
k
+
1
,
i
∣
i
=
1
,
2
,
⋯
,
B
}
 based on the previous trajectory using a Reason1
1
Prompts of Finish and Reason actions are illustrated in Appendix A
 node:

s
k
+
1
,
i
∼
π
0
⁢
(
[
s
0
⊕
s
1
⊕
⋯
⊕
s
k
]
∣
x
)
(2)
where 
⊕
 is the operation to connect two steps using the coherence phrase 
t
s
, 
π
0
 is the base policy model, 
x
 is the original input prompt and 
s
k
+
1
,
i
 is sampled with a high temperature (1.0) to harvest a diverse search space. To control that each reasoning step is atomic enough for the whole thought path, we set the stop tokens as Step k+2. Subsequently, the 
B
 nodes 
{
T
i
=
(
[
s
0
,
s
1
,
⋯
,
s
k
,
s
k
+
1
,
i
]
,
0
,
∅
,
T
c
,
0
)
∣
i
=
1
,
2
,
⋯
,
B
}
 are added to 
T
c
 as children nodes.

Node Rollout

As the PRM is not yet available during bootstrapping, the rollout process is mainly conducted using simulation to obtain the estimated value for a chosen node. The process is important since an accurate value estimation is not only crucial for the PRM training but also impacts the priority of node expansion in future exploration. To this end, for a chosen unvisited node 
T
c
, we set a simulation budget 
L
=
min
⁡
(
L
min
,
L
0
k
)
 where 
k
 is reasoning step counts of 
T
c
, to encourage sufficient simulation trials when the known reasoning path is short, but expect to see an accurate reasoning result when the trajectory is long enough. After determining the simulation attempt counts, we prompt the policy model 
π
0
 to directly output the answer 
L
 times conditioning on the reasoning trace:

a
c
l
∼
π
⁢
(
x
,
[
s
0
,
s
1
,
⋯
,
s
k
]
)
(3)
where 
l
∈
[
1
,
L
]
 and 
a
c
l
 is the 
l
-th simulated answer. The accuracy of the 
L
 simulations 
a
⁢
c
⁢
c
=
1
L
⁢
∑
l
=
1
L
𝟙
a
c
l
=
y
 is assigned as the value of 
T
c
. For the Finish node, the rollout value is computed by comparing the derived answer and the ground truth and an 
0
/
1
 value is assigned. In terms of inference, the value is obtained by calling the PRM. We elucidate this process in §2.4.

Backpropagation

After obtaining the value for the selected node, we conduct value back-propagation starting from 
T
c
 till the root node, updating all tree node values among the trace. This process aims to use more accurate value estimation in the deeper layer to update early nodes’ values so that these values serving as PRM’s optimization labels become synchronously precise. Specifically, for the node among the trace from the root node to 
T
k
, we update its visits 
n
k
 and 
v
k
 as follows:

n
k
=
n
k
+
1
(4)
v
k
=
1
2
⁢
(
v
k
+
∑
c
⁢
h
v
c
⁢
h
⋅
n
c
⁢
h
∑
c
⁢
h
n
c
⁢
h
)
(5)
Note that this update process considers not only the child value, but also incorporates the value of itself. This operation guarantees that the parent node will never have the same value as its children, which considers both correctness and completeness for the evaluation of a reasoning step.

Termination of Search

Due to computational limits, we cannot afford excessive computing resources to fully expand the tree for a training instance. Therefore, for a single reasoning trajectory, apart from the circumstance that the node is explicitly assigned as Finish due to a relatively high value, we propose to prompt the policy model 
π
0
 to self-generate the Finish signal. Specifically, we prompt the model to determine which actions to take as the next step in the Node Expansion stage. For the 
B
 children, we first prompt the model to generate 
B
 actions from [Reason,Finish]. If the model is deterministic about the reasoning process, it opts to generate consistent Finish actions. Generation of more than one Reason actions indicates that the policy model is still of low confidence about the termination of reasoning. Otherwise, we also assign a Finish action for the expanded node and take the corresponding prompt template for thought generation. A finish node will never participate in the later node expansion so the search costs are reduced. The whole tree is terminated once the number of Finish nodes reach a pre-defined node limit.

2.2 Policy Model Fine-tuning

The policy training mainly leverages the correct nodes 
T
k
1
 which are assigned a value 
1.0
 during rollout and corresponding reasoning trajectories gathered before: 
D
π
=
{
(
T
k
1
,
[
s
0
⊕
s
1
⊕
⋯
⊕
s
k
]
)
}
. These correct reasoning traces are supervised fine-tuned to deduce a self-improved policy model:

ℒ
π
=
1
L
k
⁢
∑
i
=
1
L
k
−
log
⁡
p
π
⁢
(
y
i
|
x
,
y
<
i
)
(6)
where 
y
i
 is the 
i
-th token of the reason trajectory and 
L
k
 is the total length of the trajectory.

2.3 PRM Fine-tuning

A satisfactory PRM should have the following properties: (1) For a set of reasoning steps starting from the same reasoning trace, it should differentiate between each step through their rollout values; (2) for a correct leaf and incorrect leaf, it should tend to assign a large value for the correct leaf and a low value for the incorrect leaf.

Dataset Collection

Therefore, we gather all finish nodes and their corresponding traces: 
{
(
T
k
j
,
[
s
0
,
s
1
,
⋯
,
s
k
j
]
,
v
k
j
)
∣
k
=
1
,
2
,
⋯
,
K
;
j
=
1
,
2
⁢
⋯
,
J
}
, where 
K
 is the max depth of the evolved tree, 
J
 is the max width of the 
j
-th layer, and 
v
k
j
∈
(
0
,
1
)
. The reasoning traces
[
s
0
,
s
1
,
⋯
,
s
k
j
]
 are assembled with “Step k:”. This naturally enables 
V
θ
 to distinguish between siblings inheriting from the same parent node: 
T
k
j
1
 and 
T
k
j
2
 with distinct rollout values 
v
k
j
1
 and 
v
k
j
2
. To realize the second property, 
V
θ
 should not be biased by the distribution of value 0 and value 1, since a big difference between these two sparse values tends to mislead the 
V
θ
 to assign either 0 or 1 for all leaf nodes, instead of learning the intrinsic semantic correctness. To this end, we tallied the correct and incorrect finish nodes: 
{
T
k
0
∣
v
T
k
=
0
}
 and 
{
T
k
1
∣
v
T
k
=
1
}
, and randomly sampled elements
min
⁡
(
|
{
T
k
0
}
|
,
|
{
T
k
1
}
|
)
 from these two sets, maintaining a balanced distribution of the nodes of correct and incorrect reasoning: 
{
T
^
k
0
}
 and 
{
T
^
k
1
}
. The sampled nodes are combined and form the final 
V
θ
 tuning set: 
D
V
θ
=
{
T
^
k
0
}
∪
{
T
^
k
1
}
.

Learning objective

Previous works in the Math domain choose to directly learn the rollout value (Zhang et al., 2024a) or learn the pair-wise ranking preference (Guan et al., 2025). However, in our work, we choose to learn the prediction of the correctness probability of an intermediate step using a 2-class cross-entropy loss. The PRM 
V
θ
 is tuned using the same policy model architecture, with the language model head replaced by a token classification layer with a cross-entropy loss at the end of each step. Although Zhang et al. (2025) suggests that the PRM label should be set to 1 (a hard label) once the rollout score is above zero, empirically we borrow the insight from label-smoothing (Szegedy et al., 2016), where a soft label groups positive internal steps tightly in the representation space. Specifically, we set the label 
y
k
 as the original node rollout value, which is a float number between 0 and 1, and optimize the 
V
θ
 using the following loss function:

ℒ
V
θ
=
1
|
D
V
θ
|
⁢
∑
T
k
∈
D
V
θ
y
k
⁢
log
⁡
y
^
k
+
(
1
−
y
k
)
⁢
log
⁡
(
1
−
y
^
k
)
(7)
where 
y
^
k
 is the predicted probability of the given step and 
y
k
 is the label. This soft-label training, not only encourages 
V
θ
to cluster preferred and dis-preferred steps, but also prevents the learning of fuzzy labels (rollout value around 0.5)

2.4 PRM-guided Decoding

A well-trained PRM can serve as an oracle quality verifier, to select both suitable intermediate reasoning steps as well as complete reasoning trajectories. Referring to the inference scaling law (Wu et al., 2024d), enlarging the inference token budget is a deterministic way to enhance the downstream reasoning performance. To balance the advantages of enlarging the inference budget and unwillingly reduced efficiency, we propose a new decoding strategy that relies on the well-configured PRM to select the output from sampled candidates.

PRM guided Vote-Sum (P-VS)

This method employs 
π
 to sample 
M
 candidate responses 
{
y
m
∣
m
=
1
,
2
,
⋯
,
M
}
 given an input problem 
x
 and uses the PRM 
V
θ
 to select the response whose answer is estimated to have the highest values in total. Specifically, for a simple output 
y
m
, we split it into 
K
 steps 
{
s
m
k
∣
k
=
1
,
2
,
⋯
,
K
}
 with our pre-defined step separator “
\
n
\
nStep k: ”. The PRM assigns a score 
q
∈
[
0
,
1
]
 for each internal step 
{
q
m
k
=
V
θ
⁢
(
s
m
1
⊕
s
m
2
⊕
⋯
⊕
s
m
k
)
∣
k
=
1
,
2
,
⋯
,
K
}
. The overall response score 
v
y
m
takes the minimum value of the score chain (Lightman et al., 2023) or the last value (Zhang et al., 2025). The Vote-Sum strategy comprehensively considers the occurrence of semantically equivalent outputs but also the confidence score predicted by 
V
θ
, which is computed as such:

a
v
=
{
(
a
1
,
v
a
1
)
,
(
a
2
,
v
a
2
)
,
⋯
,
(
a
n
,
v
a
n
)
}
(8)
y
t
⁢
a
⁢
r
⁢
g
⁢
e
⁢
t
=
argmax
y
m
∈
{
y
j
∣
a
y
j
=
argmax
a
i
v
a
i
,
(
a
i
,
v
a
i
)
∈
a
v
}
v
y
m
(9)
where 
n
≤
M
 is the number of answers with different semantics obtained from the 
M
 responses, 
v
a
n
=
∑
m
𝟙
a
y
m
=
a
n
⋅
v
y
m
is the sum of values of all semantically equal traces.

Refer to caption
Figure 2: Overview of the used seed datasets.
3 Data Statistics

A slow-thinking system in medical scenarios should both excel at answering exam questions, but also handling real-world clinical scenarios. To this end, we extend beyond traditional multiple-choice questions, and incorporate more diverse datasets into the seed evolution datasets. We gather four types of clinical tasks, with each task containing different medical datasets:

1. Long Context Question Answering: This dimension is to enable MedS3 to capture useful information from the given context and use long-chain reasoning to answer problems. This dimension covers BioMRC (Pappas et al., 2020), HeadQA Topic Classification (Vilares and Gómez-Rodríguez, 2019; Wu et al., 2024b), and DDX-Plus (Tchango et al., 2022).
2. Knowledge-Intensive Question Answering: This dimension is expected to enable MedS3 to use long-chain reasoning to answer knowledge-intensive problems. This dimension covers MedQA (Jin et al., 2021), MedMCQA (Pal et al., 2022), and PubMedQA (Jin et al., 2019).
3. General Question Answering in Medical: This part leverages general data in bio-medicine domains to enhance the generality of MedS3, which includes SciQ (Welbl et al., 2017), Evidence Inference (DeYoung et al., 2020) and Head QA (Vilares and Gómez-Rodríguez, 2019).
4. Medical Natural Language Inference: This dimension is considered to opt MedS3 to discriminate biomedical research concepts and corresponding descriptions. This dimension covers Healthfact Classification (Kotonya and Toni, 2020), Medical Question Pair (MQP; McCreery et al. (2020)), and catalonia-independence-corpus (CIC; Zotova et al. (2020)).
5. Diagnosis Question Answering: This dimension is related to real-world clinical scenarios, including disease diagnosis and classification and drug related questions. We choose Covid-19 Classification (Lab, 2020), Drug-Dose Extraction and Adverse Drug Event Classification (Huynh et al., 2016; Wu et al., 2024b).
For each involved dataset, we randomly select 500 items and form a seed dataset with 8,000 instances. We show the dataset statistics in Fig. 2. For more detailed description of each dataset, please refer to Appendix B.

Knowledge QA	General Medical QA	Long-Context QA	Diagnosis QA	NLI		Models	MedQA	MedMCQA	HealthFact	PubMedQA	BioASQ	MMLU	BioMRC	DDXPlus	PubHealth	DrugDose	SEER	Avg.	Proprietary language models	GPT-4o-mini	75.81	67.58	65.24	47.80	83.01	83.79	66.85	54.00	59.14	73.91	54.54	66.52	GPT-3.5-turbo	59.31	58.12	67.85	37.40	74.11	71.11	56.22	39.05	57.84	86.96	73.61	61.96	Open-source language models	Qwen2.5-7b	55.54	54.12	52.69	53.40	73.62	74.38	56.48	31.25	57.11	60.87	33.07	54.78	Llama3-8b	57.50	55.92	70.88	56.40	75.73	68.55	56.50	35.30	64.09	73.91	47.07	60.17	Llama3.1-8b	61.51	57.42	63.97	59.00	71.36	72.52	55.60	19.00	61.82	73.91	52.62	58.98	QwQ	68.89	61.03	66.08	48.60	73.62	74.18	79.76	45.40	63.36	39.13	37.26	59.76	Open-source medical models	MMedS-Ins	53.57	48.24	69.64	56.60	77.35	50.86	31.47	97.53	54.26	95.65	97.93	66.65	MedLlama3	55.85	59.36	68.10	66.40	84.63	70.08	47.97	22.50	62.39	69.57	50.69	59.78	Med42	50.20	49.70	81.57	55.40	74.76	61.43	57.26	31.35	59.14	65.22	37.14	56.65	OpenBioLLM	50.20	50.56	53.28	41.40	47.73	61.69	27.46	16.55	18.77	34.78	46.48	40.81	UltraMedical3-8b	68.89	61.82	72.73	51.60	80.58	75.08	45.18	36.70	66.13	60.87	24.55	58.56	UltraMedical3.1-8b	70.93	62.78	70.20	56.40	77.18	76.43	54.26	31.55	59.14	56.52	45.86	60.11	Open-source slow-thinking medical models	HuatuoGPT-o1	62.53	59.31	66.08	69.20	87.70	70.53	50.98	40.20	24.61	56.52	46.85	57.68	MedS3													    COT	65.91	60.55	64.73	56.80	78.48	75.66	55.84	51.65	57.03	73.91	48.97	62.68	    SC	70.93	64.21	70.37	58.20	79.13	79.63	63.66	57.00	64.42	86.96	52.19	67.88	    PRM	71.88	65.20	79.97	59.60	80.10	79.50	77.12	65.20	73.03	95.65	58.36	73.24 
Table 2: Experiment results in 11 medical datasets among four types of models. We highlight the best results with bold and underlines the second-best results. MedS3 achieves superior performances on real-world clinical datasets, especially in DrugDose and DDX-Plus.
4 Experiments

In this section, we comprehensively evaluate the proposed MedS3 on various downstream domains, including both in-domain and out-of-domain datasets.

4.1 Experiment Setups

Training and Evaluation

We choose Llama3.1-8B-Instruct as the initialization of both the policy model and PRM model. We select MedQA (Jin et al., 2021), PubMedQA (Jin et al., 2019), MedMCQA (Pal et al., 2022), PubHealth (Kotonya and Toni, 2020), BioMRC (Pappas et al., 2020), HealFact Classification (Kotonya and Toni, 2020), Drug Dose Extraction (Huynh et al., 2016), DDX-Plus (Tchango et al., 2022) as the in-domain evaluation benchmarks, the medical parts of MMLU (Hendrycks et al., 2021), BioASQ (Tsatsaronis et al., 2012) and SEER Classification (Dubey et al., 2023) as the out-of-domain evaluation sets. We provide three different decoding strategies for MedS3, including CoT, Self-Consistency and our proposed PRM guided Vote-Sum. COT (Wei et al., 2022) directly prompts models to generate a long reasoning chain and outputs the answer with “The answer is {answer}” for the convenience of answer extraction. Self-Consistency (Wang et al., 2023) generates 
N
=
16
 samples for a given problem, and we select the one whose answer appears most times among the 
N
outputs. For PRM-guided Vote-Sum, we generate 
N
=
12
 samples for datasets where SC has already achieved a high score (MedMCQA, Med-MMLU and BioASQ) and generate 
N
=
16
 samples for other datasets. This is reasonable since not all generated samples are of high quality due to the sampling parameters top_p. We manually filter some low-probability samples to prevent them from disturbing the Vote-Sum computation. The hyperparameters of synthesis, self-training and evaluation are presented in Appendix C.

Baselines

We choose the following categories to serve as baselines: (1) Proprietary general LLMs, including GPT-3.5-turbo (OpenAI, 2022) and GPT-4o-mini (OpenAI, 2023); (2) Open-sourced general LLMs, including Llama 3 8B, Llama 3.1 8B (Dubey et al., 2024) and Qwen2.5 7B (Yang et al., 2024), and QWQ-preview-32B (Team, 2024); (3) Open-sourced Medical LLMs, including MedLlama 3 8B2
2
https://huggingface.co/ProbeMedicalYonseiMAILab/medllama3-v20
, MMedS-Ins-Llama-3-8B (Wu et al., 2024b), Med42 (Christophe et al., 2024), OpenBioLLM (Ankit Pal, 2024), and UltraMedical3-8B and UltraMmedical3.1-8B (Zhang et al., 2024b). We also compare our method with HuatuoGPT-o1-8B (Chen et al., 2024).

4.2 Main Results

We present the comprehensive experiment results in Table 2. The results unveil that most prior medical LLMs show superior results in traditional medical benchmarks (MedQA or PubMedQA); while such superiority cannot generalize to out-of-distribution real-world clinical benchmarks (DDXPlus or DrugDose), which results in their sub-optimal overall performance compared to Llama3-8B. This suggests that without choices as prior knowledge, current medical models have a limited application scope. In contrast, our MedS3 is not optimized exclusively for multiple-choice medical datasets; therefore, MedS3 achieves the best performance among all open-sourced competitions. As an 8B model, MedS3 achieves +13.07 average performance gains with respect to the base model in the overall assessment, which not only outperforms medical-oriented models as well as GPT-4o-mini with the PRM as a supervisor. Although MedS3 lags behind on BioASQ and PubmedQA compared to HuatuoGPT and MedLlama3, MedS3 shows superior performance on reasoning-intensive benchmarks, including MedQA and MedMCQA, as well as clinical benchmarks. This verifies that MedS3 learns medical reasoning philosophies and the clinical deduction process. Another model MMedS-Ins, which focused on clinical usage, directly post-pretraining and fine-tuning on millions of medical corpus, harvests superior performance on in-domain clinical tasks, such as SEER and DDXPlus. However, directly fine-tuning on question-answer pairs inevitably makes the model loss the ability to output long responses, which is extremely important in other reasoning tasks. As a result, although the over-fitting model achieves superior performance in three clinical benchmarks, our model MedS3, fine-tuned with a self-evolve pipeline, hardly impairs the internal distribution and hence possesses comprehensive clinical usage.

5 Analysis

Settings	Traditional QA	Clinical Tasks	Mean	
MedQA
 	
MedMCQA
Healthfact
DDXPlus
SEER
Base	61.51	57.42	63.97	19.00	52.62	50.90	BoN-min	67.64	59.20	73.57	52.40	41.24	58.81	BoN-prod	65.51	60.00	75.51	57.30	47.67	61.20	BoN-last	60.17	58.00	78.37	65.10	61.67	64.66	PRM-Vote-Sum	71.01	65.20	79.97	65.20	58.3	67.95 
Table 3: Comparison of Vote-Sum with other methods utilizing PRM under the same token budgets. Our proposed vote-sum achieves the best performance, while recently proposed BoN-last is secondary to us.
5.1 Selection of Vote-Sum

In this section, we compare our proposed decoding method PRM-guided Vote sum to the previously widely adopted PRM decoding method: Best-of-N (BoN). We compare three variants of BoN, including BoN-min (Lightman et al., 2023), BoN-prod (Lightman et al., 2023) and BoN-last (Zhang et al., 2025) The first method, BoN-min, takes the minimum step value of a reasoning trajectory as the estimation of the whole sequence. The second method, BoN-prod, takes the production of step values of a whole trajectory as the sequence value prediction. The last method is recently proposed, which deems that PRM trained with Monte-Carlo-rollout (MC-rollout) values are basically an ORM and hence only the last step’s prediction is authentic. We select two traditional QA tasks and three clinical tasks to conduct comparison and show results in Table 3. A very interesting finding is that in two datasets of traditional QA, BoN-last achieves extremely bad results, but shows great performance on par with Vote-Sum in clinical tasks. We hypothesize that clinical tasks and Olympia-level math problems are both difficult for Llama3.1-8B, and hence the findings that PRM behaves more like an ORM transfer successfully to clinical tasks. On the other hand, on relatively advantageous tasks, MC-rollout value encodes values of both future and current steps, which leads the PRM to perform as a human-labeled PRM to distinguish a bad step from candidates. As a result, in traditional medical tasks, we use the minimum value of a whole sequence to represent the whole sequence’s correctness, while in clinical tasks, we use the last step’s value to serve as the whole response’s score.

Refer to caption
Figure 3: Overview of the used seed datasets.
5.2 Inference-Time Scaling

In this section, we discuss whether MedS3 can benefit from more inference tokens. We sample 
n
=
2
,
4
,
8
,
12
,
16
 samples for a given problem, and conduct PRM-Vote-Sum to select the most plausible response. We select five clinical datasets (Healthfact, DDXPlus, SEER, BioMRC, Pubhealth) to illustrate the scaling law, and plot the results in Fig. 3. We observe that the first 4 samples provide significant performance improvements, showing a satisfactory tradeoff between the tokens count and the performance gain. Although more generations bring less significant performance gains, the increasing trend never slows down except for the Pubhealth dataset, illustrating unbounded scaling potentials. For the sake of large-scale evaluation, we finalize the search budget to 16 across all experiments.

6 Related Works

With the success of the generalist LLMs, their advancements in both clinical and biomedical scenarios have shown significant promise. Numerous previous works focus on developing medical-specific LLMs, which are now being increasingly adopted across various clinical settings. These medical LLMs generally follow three main approaches:

Continual Pre-training Medical LLMs

These types of medical LLMs Xu (2023); Chen et al. (2023c) are developed on the advantage of generalist LLM and attempt to inject domain-specific knowledge and expertise through continual pre-training techniques. Such type of methods usually require significant computational resources. For example, 3 billion training tokens are used in HuatuoGPT-II (Chen et al., 2023a) and PMC-Llama (Wu et al., 2024a) even requires more than 75 billion tokens. However, results in recent works (Qiu et al., 2024) show that the benefits of continued pre-training are diminishing as the capabilities of the generalist LLMs improve.

Fine-tuned Medical LLMs

Compared to continuous pre-training, fine-tuning is a more efficient approach. It can rapidly adapt to medical scenarios and perform the relevant tasks effectively when the base LLMs are sufficiently powerful. (Ankit Pal, 2024; Christophe et al., 2024; Zhang et al., 2024b) Specifically, Liao et al. (2024a) develops a two-stage method which can decouple the knowledge-injection and clinical alignment procedure during the fine-tuning process to prevent the ‘alignment-tax.’ Wu et al. (2024c) collects a wide range of medical language processing tasks spanning 19 task categories and 122 unique clinical tasks to improve the LLMs’ capacities on various downstream clinical tasks.

Slow-Thinking Medical LLMs

With the significant achievements of the o1 (Jaech et al., 2024) in complex reasoning tasks, previous works show the potential advantage of the o1-like models in medical tasks (Xie et al., 2024; Xu et al., 2024a; Nori et al., 2024). Based on these, previous works develop the slow-thinking medical LLMs with distillation: Huang et al. (2025) directly learn the reasoning trajectory generated by o1 and Chen et al. (2024) improving the model’s reasoning ability through o1 synthesis of reflective data and reinforcement learning. Besides, Yu et al. (2025) create a Chinese version slow-thinking medical LLMs by constructing the preference data with QwQ (Team, 2024).

7 Future Work

As a pioneering work, we have validated that small language models can self-evolve to empower themselves with strong reasoning abilities in clinical usage. There are several remaining directions to further enhance MedS3:

1. Conduct iterative evolution. Currently we are only focusing on one iteration evolution, which greatly leaves the PRM under-tuned.
2. Conduct Human-interference evaluation. MC-rollout value is verified to be not the best choice for evaluating the value of an internal step. We are eager to introduce fine-grained step label to enhance the optimization of the PRM.
3. Introduce more clinical data, not limited to close-ended generation. Currently, all the data used in MedS3 are close-ended, and the application of reasoning is not limited to such a narrow room. We intend to extend MedS3 to broader clinical tasks to make MedS3 a more useful system.
We will continue our exploration and make MedS3 more practical in medical domains.

8 Conclusion

In this paper, we present MedS3, a self-evolved slow-thinking medical language model built for universal clinical usage. We collect a small seed dataset, with only 7,000 instances but covering 16 difference realistic medical tasks, and use Monte-Carlo Tree Search to construct policy data and PRM data. We propose a new decoding method, which enables the resulting policy model to collaborate with the fine-tuned PRM model, to produce credible long-chain responses. Experiment results demonstrate that our model achieves state-of-the-art performance on eleven downstream medical benchmarks, especially in realistic clinical ones, even surpassing GPT-4o-mini with a large margin.