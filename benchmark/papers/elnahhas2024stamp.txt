From whole-slide image to biomarker prediction: end-to-end weakly supervised deep learning in computational pathology

Omar S. M. El Nahhas, Marko van Treeck, Georg Wölflein, Michaela Unger, Marta Ligero, Tim Lenz, Sophia J. Wagner, Katherine J. Hewitt, Firas Khader, Sebastian Foersch, Daniel Truhn & Jakob Nikolas Kather 
Nature Protocols (2024)Cite this article

4075 Accesses
3 Citations
50 Altmetric
Metrics details
Abstract
Hematoxylin- and eosin-stained whole-slide images (WSIs) are the foundation of diagnosis of cancer. In recent years, development of deep learning-based methods in computational pathology has enabled the prediction of biomarkers directly from WSIs. However, accurately linking tissue phenotype to biomarkers at scale remains a crucial challenge for democratizing complex biomarkers in precision oncology. This protocol describes a practical workflow for solid tumor associative modeling in pathology (STAMP), enabling prediction of biomarkers directly from WSIs by using deep learning. The STAMP workflow is biomarker agnostic and allows for genetic and clinicopathologic tabular data to be included as an additional input, together with histopathology images. The protocol consists of five main stages that have been successfully applied to various research problems: formal problem definition, data preprocessing, modeling, evaluation and clinical translation. The STAMP workflow differentiates itself through its focus on serving as a collaborative framework that can be used by clinicians and engineers alike for setting up research projects in the field of computational pathology. As an example task, we applied STAMP to the prediction of microsatellite instability (MSI) status in colorectal cancer, showing accurate performance for the identification of tumors high in MSI. Moreover, we provide an open-source code base, which has been deployed at several hospitals across the globe to set up computational pathology workflows. The STAMP workflow requires one workday of hands-on computational execution and basic command line knowledge.

Key points
STAMP (solid tumor associative modeling in pathology) is a practical workflow for end-to-end weakly supervised deep learning in computational pathology, enabling prediction of biomarkers directly from whole-slide images.

This protocol differentiates itself from others by providing a collaborative framework through which clinical researchers can work with engineers to set up a complete computational pathology project.

Similar content being viewed by others

Artificial intelligence for digital and computational pathology

Article 02 October 2023

Deep learning in cancer pathology: a new generation of clinical biomarkers

Article Open access
18 November 2020

Pacpaint: a histology-based deep learning model uncovers the extensive intratumor molecular heterogeneity of pancreatic adenocarcinoma

Article Open access
13 June 2023
Introduction
Routine histopathology slides of solid tumors are available at scale and harbor clinically actionable information1. Analyzing digitized whole-slide images (WSIs) is a complex task that requires multidisciplinary collaboration between clinicians and engineers (Fig. 1). Unfortunately, bringing these experts together is challenging, thus hindering progress in the field. Deep learning has been used to process these WSIs2, to diagnose3,4 and subtype tumors5,6 and to extract more abstract properties, such as information about the tumor prognosis7,8 and treatment response9,10.

Fig. 1: Conceptual overview of the protocol.
figure 1
The protocol starts with the definition of the clinical hypothesis and prediction target for the model (Step 1), leading to data collection, which includes assessing the number of slides (Step 2) and the quality of the data (Step 3) to establish inclusion and exclusion criteria. Subsequent steps involve defining the desired image resolution (Step 4), the cleaning and transforming of the prediction categories of the target biomarker (Step 5) and defining the strategy to validate the clinical hypothesis (Step 6). Then, evaluation of computer resources (Step 7) and setting up the configuration file (Step 8) are required to perform image pre-processing including normalization, tessellation and feature extraction (Step 9). Performing an assessment of the preprocessed images (Step 10) before the modeling steps is recommended. Setting up the modeling configuration involves defining slide and clinical files (Steps 11 and 12), defining the data splits for modeling (Step 13) and training the model on the selected validation approach (Steps 14 and 15). The model performance is then evaluated on the cross-validation (Step 16) and retrained by using all data to obtain a final model (Step 17). Next, the model is deployed and evaluated on an external cohort (Steps 18–20). For explainability purposes, spatial interpretability maps are generated (Step 21), with which the model’s technical performance is reviewed (Step 22). The associations between the model and related clinical data (Step 23) together with review of the spatial heatmaps assist the clinician in assessing the clinical relevance of the model’s predictions (Step 24). Finally, the prognostic capability of the predicted biomarker is calculated (Step 25).

Full size image

In the context of computational pathology, deep learning–based biomarkers refer to quantitative features extracted from routine pathology slides that provide prognostic or predictive information. These features are numerical vectors that have been learned by a deep learning model through the large-scale analysis of various aspects of the slide, such as the morphology of cells and their spatial arrangement within the tissue. The extraction of these biomarkers has the potential to be transformative for precision oncology, because they could lead to improved treatment for patients, rather than just optimizing workflows11. In the last few years, dozens of academic studies have shown that such biomarkers can be extracted from routine tissue slides in multiple tumor types12, such as lung cancer13,14, colorectal cancer15,16,17,18, liver cancer19, breast cancer20,21 and renal cell cancer22. These studies used a weakly supervised prediction approach, meaning they used deep learning to predict a label that is defined for a slide or a patient without the need for annotations at the pixel level.

Computational pathology research projects share a common computational approach23 (Fig. 2): they begin with a digital histopathology slide, divide it into thousands of image tiles (also referred to as ‘patches’), extract features from each tile and use a machine learning model to combine these features at the slide level for a specific prediction objective. The repeated redevelopment of core computational pathology workflows13,24,25,26,27 not only obstructs advancements in the field through time lost on standard workflows, but also causes inefficiency and risks low reproducibility because of the complex nature of the computational pipeline28. Furthermore, as computational techniques evolve, they tend to become more intricate and intertwined within complex source code, rendering them less accessible to clinicians and raising the threshold for their adoption in research.

Fig. 2: Computational workflow from WSI to patient-level biomarker prediction.
figure 2
a, Image preprocessing including WSI tessellation and hematoxylin and eosin (H&E)-staining color normalization. b, Feature extraction using self-supervised contrastive learning (CTransPath) to obtain feature vector representations for each tile in the WSI. c, Feature vectors and the target biomarker are input to the weakly supervised model, which aggregates tile feature representation by using a transformer to provide a slide-level prediction. d, Explainability of the final predictions using gradient class activation maps for model interpretability purposes. The heatmaps highlight the most relevant areas for the model’s decision making for a biomarker prediction. The red and blue percentages are examples of the model’s predicted likelihood of biomarker positivity or negativity.

Full size image

Development of the protocol
To address this issue, we have developed an end-to-end protocol for solid tumor associative modeling in pathology (STAMP), which streamlines the analysis of WSIs and enables medical and technical experts to work together effectively. Our protocol provides a comprehensive framework for going from WSI to patient-level biomarker prediction through a step-by-step guide accompanied by layperson explanations of the algorithmic and implementation details, facilitating communication and collaboration between experts with different backgrounds. We have developed an open-source software pipeline that is flexible and modular and allows for a low-barrier approach to weakly supervised deep learning in pathology. The modularity facilitates independent and asynchronous execution of each step within the STAMP workflow, mirroring the operational structure of computational pathology projects conducted with real-world data. This pipeline was used in several clinical studies led by both clinicians and engineers, including gastric cancer29,30,31, colorectal cancer32,33,34, breast cancer35, brain cancer36 and pan-cancer37,38,39 studies. It has shown state-of-the-art performance in weakly supervised computational pathology for clinically relevant prediction tasks. By following our protocol, medical and technical experts can speak the same language and use the same tools to answer highly relevant clinical questions, enabling computational pathology projects to be initiated at scale and potentially leading to new discoveries toward improved patient outcomes.

The novelty of the protocol lies within its ease of use for clinical researchers to set up a complete computational pathology project, as well as using the latest deep learning techniques within a general applicable workflow to map the non-linear phenotypic-genotypic relationships directly from routine WSIs. Specifically, we use a state-of-the-art pretrained self-supervised learned histology feature extractor40 and Transformer architecture33 for the downstream weakly supervised classification task. We provide technical details of the model architecture and implementation (Supplementary Text 1). This protocol has shown to be broadly applicable to numerous solid tumor types and various categorical or continuous genetic biomarkers29,30,31,37,38,39. The protocol is accompanied by an open-source codebase (available at https://github.com/KatherLab/STAMP). No code customization is required to use this protocol on new datasets and tasks, only a change of values inside the accompanying configuration file.

It is important to emphasize that our protocol exclusively enables associative modeling between input features and outcome targets, rather than facilitating causal inference. In other words, it supports the identification of relationships between tissue phenotype and biomarkers without determining whether specific features directly cause particular outcomes. Causality in digital medicine is complex and an ongoing area of research, demanding a multidisciplinary environment encompassing domain expertise for accurate assumptions41.

Overview of the procedure
The protocol consists of the following five overarching stages: (i) problem definition, (ii) data preprocessing, (iii) deep learning, (iv) model evaluation and (v) clinical translation. These stages are executed as a collaborative project that is partly clinician led and partly engineer led. The stages are divided into three distinct phases: orchestration, execution and interpretation (Fig. 1). Orchestration involves problem formulation and data curation (Stage 1, Steps 1–6). It requires clinical expertise to define the research question, eliminate low-quality WSIs and transform the biomarker data. Data transformation may involve steps such as removing or merging categories within a biomarker, binning continuous variables and combining several biomarkers into a single target variable. The clinician should be supported by a statistician through the provision of fitting metrics and statistical tests required to answer the medical questions at hand, which will influence the minimum sample size required to start the study. The execution phase is primarily focused on technical aspects, ranging from image preprocessing to modeling and technical evaluation (Stages 2 and 3, Steps 7–22), which should be supervised by an engineer with a solid understanding of image processing, deep learning techniques and the technical evaluation of deep learning models. The engineer should be supported by information technology experts for the deployment of the software on a computing instance, as well as handling the moving of imaging data between file systems. The interpretation phase, which encompasses clinical evaluation and explainability (Stages 4 and 5, Steps 23–25), is a collaborative effort between clinicians and engineers. During this phase, clinicians concentrate on the medical relevance and interpretability of the results, whereas engineers focus on technical aspects, such as overfitting, which may affect the results. The clinicians and engineers are supported by data scientists who present and quantitatively evaluate the model, by statisticians who perform statistical analyses and by pathologists who interpret phenotypic patterns that are highlighted by the model. By leveraging the multidisciplinary expertise across all three phases, the protocol can be effectively executed and provide meaningful results. To ensure effective interdisciplinary collaboration for this protocol, a general guideline is to have a clinician lead the orchestration and interpretation phase, while an engineer leads the execution phase. In addition, a glossary of common terms is provided to facilitate communication between clinicians and engineers (Table 1).

Table 1 Glossary of commonly used terms for communication between clinicians and engineers
Full size table

Limitations
It should be noted that the resulting models’ performance may be limited in cases of low data availability (<100 patients) or in instances of severe class imbalance between positive and negative cases (<5 positive cases) (Fig. 3). The protocol does not augment the data to increase data volume, because of the readily abundant information present within routine WSIs of gigapixel resolution, meaning they contain more than a billion pixels. However, images have been augmented during training of the self-supervised learned histology feature extractor to improve model robustness40. A recent study showed that additional image augmentations do not affect the downstream classification performance42 and are thus not included within the standard protocol software. Our codebase that accompanies the protocol has a limitation in terms of the supported file types for WSIs. Specifically, it supports only file types that are compatible with the OpenSlide library43. Not all image file types are supported, and this may affect the applicability of our protocol to some datasets. The supported file types include SVS (ScanScope Virtual Slides), MRXS (3DHISTECH MIRAX), NDPI (Hamamatsu) and other file types that can be transformed into pyramidal Tag Image File Format (TIFF) files. It is important to consider file type compatibility when selecting datasets to apply our protocol. Moreover, the protocol assumes access to sufficient computational resources to preprocess the WSIs, train models and compute heatmaps. The workflow can be initiated only from a command line interface (CLI), therefore requiring at least a basic understanding of running a command in such an interface. Altering the models’ hyperparameters requires knowledge of Python programming, because not every parameter can be adjusted through the CLI (Supplementary Text 1).

Fig. 3: Factors influencing the required sample size in computational pathology projects.
figure 3
a, A slide with pen marks on the tissue and a dirty glass surface requires a higher sample size for modeling than a clean slide. b, Larger tissue slides, such as resections, contain more information than a biopsy for deep learning to find patterns. Therefore, large resections require fewer samples to model than do small biopsies. c, Strongly supervised learning requires less data than weakly and self-supervised learning, because labels are provided and predicted for each individual tile, rather than a single label for the entire slide. Labels for each individual tile are annotation intensive but allow for the model to learn more specific patterns with fewer slides. d, A larger contrast between phenotypes of the positive and negative instance of a biomarker is easier to model and therefore requires fewer samples than having instances with a similar phenotype. e, Rare positive instances of a biomarker, thus having an imbalanced dataset, require more samples for modeling than having a balanced distribution of the labels or annotations. f, Larger, more complex models like transformers tend to have less inductive bias, making them adaptable to various tasks. Less inductive bias requires substantially larger amounts of data to model. In contrast, smaller models, although simpler and more biased, tend to generalize well on smaller sample sizes because of their more constrained nature. The wedges at the bottom represent the required sample size for the data and modeling scenarios shown in panels a–f. A flow is present across the panels: in panels a,c and e, the left-most situation indicates a need for more samples, whereas the right-most situation suggests that fewer samples are required. Conversely, in panels b,d and f, the left-most situation suggests fewer samples, and the right-most situation indicates a need for more samples.

Full size image

It should be noted that this protocol is meant as a lean and low-barrier entry for weakly supervised deep learning in computational pathology. Many extensions have been added over time, and these enable the user to choose between pretrained models for feature extraction, use different models for downstream fine-tuning tasks, opt between classification and regression, perform multi-task learning and choose different ways of generating spatial attention heatmaps. For the purpose of simplicity, those features are not present in the base protocol but are made available open source (https://github.com/KatherLab) as extensions for advanced users. The protocol is maintained in GitHub, which facilitates version management of the source code and corresponding user instructions. This enables future versions of STAMP to adapt to trends in computational pathology, such as adopting Digital Imaging and Communications in Medicine (DICOM) as an input format for WSIs, new modeling architectures and new datasets and research questions, while retaining its robustness and uniformity in usability. The software used for problem formulation and clinical translation are not included in the STAMP computational workflow, because this is a process that is often performed by using tools such as Excel, programming languages such as R and pathology slide viewers such as QuPath44.

It is important to note that there is a substantial gap between models generated through STAMP and a tool that is approved by regulatory bodies for usage in clinical practice. First, the regulatory approval requires technical documentation and thorough validation, which is not explicitly supported in this protocol. Second, there are technical details that should be considered to potentially achieve an improved and more robust model performance through hyperparameter tuning for specific use cases. Finally, spatial interpretability maps used within the protocol should not be relied upon as a leading evaluation metric because of confirmation bias of the observer, but rather used for troubleshooting to improve model performance45. The STAMP protocol, however, is limited to proof-of-concept studies for research purposes only, aiming to provide a meaningful starting point upon which studies in the domain of computational pathology can be built.

Comparison with other protocols
In recent years, several general-purpose enabling software supporting WSI deep learning protocols, such as PyTorch46, MONAI47, OpenSlide43, LibVips48 and RAPIDS cuCIM (https://rapids.ai), have been developed. These tools form the basis for loading WSIs and providing an environment to create deep learning models. Building on top of aforementioned tools are domain-specific adaptable software, such as HistoQC49, FastPathology50, TIAToolbox51, CLAM52 and STAMP (Fig. 4a). HistoQC is a software tool used in digital pathology to assess the quality of WSI slides in an automated manner, but it does not provide tools for further computational analysis related to biomarker prediction. The FastPathology platform provides a platform with a visual interface capable of supervised deep learning for segmentation and object detection, but it does not provide modular software for asynchronous execution of processing steps, which is preferred for the iterative and explorative nature of computational pathology projects. The TIAToolbox software provides a toolbox for processing and modeling of WSIs, offering a large range of tools including WSI pre- and post-processing, instance segmentation and weakly supervised deep learning. Similarly, the CLAM software encompasses several tools for pre- and post-processing of WSIs but is focussed on weakly supervised deep learning. Furthermore, there has been a noticeable increase in the development of proprietary single-application software within the medical field53. This involves transforming open-source, domain-specific, adaptable software applications into products that have received certification from regulatory authorities like the US Food and Drug Administration and the European Conformité Européenne for In Vitro Diagnostic standards, such as a tool for prediction of microsatellite instability (MSI) status in CRC54, which is developed with a similar philosophy as proposed in prior research15 and is analogous to the STAMP computational workflow.

Fig. 4: Positioning of the STAMP software.
figure 4
a, The STAMP protocol provides pathology-specific software, built on top of general-purpose software, which has been reduced to core technical features to provide state-of-the-art computational pathology methods, developed for usage by a clinical user base. b, The STAMP protocol enables the answering of computational pathology research questions in the intermediate phases of the cancer patient journey, focusing on use cases such as diagnosis, prognostication and treatment response prediction in a weakly supervised manner.

Full size image

A recent review identified three general weaknesses of currently available computational pathology workflows28: the lack of experiment reproducibility when using the code, the lack of real-world translation of results and the lack of supporting documentation to reuse the code. The review underlines that the currently available workflows are missing supporting documentation alongside the codebase, hindering researchers without a computational background from reusing the provided software. The aforementioned workflows have in common that they are technically robust techniques that require programming knowledge to use but lack thorough documentation for non-computational researchers to interact with their software and conceptualize a computational pathology study.

Our protocol provides lean software for WSI processing focused on weakly supervised deep learning in an end-to-end and generally applicable manner, which requires no programming knowledge to use. We contribute to existing software by putting emphasis on lowering the entry barrier for use by clinicians (Fig. 4), while facilitating the use of state-of-the-art computational methods that have been used in a plethora of clinician-led29,30,32,36 and engineer-led31,33,35,37 studies. Moreover, our protocol goes beyond providing a software workflow by including guidance on problem formulation and clinical translation, providing a guideline for end-to-end computational pathology projects using real-world data. Through our protocol, we aim to enable fruitful collaborations between clinicians and engineers to work on research questions in computational pathology, promoting the development of more certified diagnostics products that lower the burden on the healthcare system and improve patient outcomes.

Experimental design
Implementation of our protocol requires WSIs, accompanied by related clinical information and molecular data. To exemplify the protocol, the phenotypic features of colorectal cancer (CRC) WSIs are mapped to the genotypic features of mismatch repair (MMR) genes related to MSI. This cancer type and clinical endpoint were chosen because of their known strong phenotypic-genotypic correlation15,32,33,55. The first step of the protocol is to define the clinical hypothesis to be tested and the target to be predicted. In this case, the clinical hypothesis is therefore defined as ‘deep learning can predict MSI directly from hematoxylin and eosin (H&E)-stained WSIs’, with the target to be predicted being the MSI status (see Step 1).

Here, we used the open-source datasets from The Cancer Genome Atlas56 (TCGA) and Cancer Proteomics Transcriptomic Tumor Analysis Consortium57 (CPTAC) efforts. The minimum number of patients required for artificial intelligence (AI) analysis of WSIs is dependent on various factors (Fig. 3). First, the required size of the dataset depends on the strength of the correlation between tissue phenotype and biomarkers, and larger datasets may be necessary for problems with weak correlations between phenotype and genotype. On the other hand, smaller datasets may be sufficient for problems with strong correlations. Modeling the difference between positive and negative instances of a biomarker becomes easier when there is a greater contrast in their phenotypes. Consequently, fewer samples are needed compared to instances in which the phenotypes are more similar. Second, the number of positive cases in the dataset is a crucial factor to consider. For problems that involve rare mutations or events, larger datasets are necessary to ensure adequate representation of these positive cases and create an overall more-balanced dataset of the classes to predict. Conversely, for more common problems, smaller datasets may suffice. Third, the quality and size of the tissue play an important role. Finally, model complexity and inductive bias are inversely related, with larger models like transformers having less inductive bias, enabling adaptability to various tasks at the cost of requiring substantially more data. Conversely, smaller, more-biased models generalize well on limited data because of their constrained nature. These factors must be carefully considered when determining the appropriate dataset size for AI analysis of WSIs in computational pathology. In addition, the selection of statistical tests and their power calculations are influenced by the available sample size, which is crucial for performance evaluation in later stages of the protocol.

The definition of good data quality for this protocol is based on the histology slide and corresponding biomarkers (see Step 3). An ideal slide for deep learning purposes would be a tumor resection without blurry regions, penmarks or dirt. Resections contain more tissue and therefore have more data that can be learned from by a model. A biopsy is usually a substantially smaller tissue sample, which might therefore require more samples for the model to learn complex patterns. However, the clinical hypothesis ultimately guides what type of surgery material is preferred, because it is crucial to train a model on a similar type of data to that on which it is intended to be deployed.

For the corresponding molecular biomarkers, having data that were sequenced from the subsequent tissue slice used for the slide is preferred, so that the genetic information is from tissue as close as possible to the slide that we observe. For biomarkers established by pathologists, the slide used for diagnosis is also used for the analysis. Biomarkers to be predicted can also be transformed in various ways (see Step 5). For example, the MMR genes (MLH1, MLH3, MSH2, MSH3, MSH6, PMS1 and PMS2) could be used as individual targets for the prediction of MSI status. These targets, because of the output of sequencing methods, are continuous targets in their unprocessed form. In this scenario, one could opt for the transformation of binning the continuous targets into low-, medium- and high-expression classes for each of the genes. Another subsequent transformation could be to merge the low- and medium-expression classes of the genes, turning the problem into a binary classification problem between differentiating between low- and medium-expression versus high-expression classes. Another transformation could be to merge the different targets related to MMR, creating a single target signature from the seven aforementioned gene expressions.

The patients should be allocated in a manner that enables validation of the clinical hypothesis (see Step 6). Splitting patients into a train set (80% of samples) and test set (20% of samples), stratified by the site that processed their tissue, is recommended. This creates a more transparent perspective of the model’s generalizable performance on samples that came from external sources, which are potentially prepared with a varying staining protocol and scanned with a different scanner. Moreover, it is crucial that the positive and negative cases (i.e., high MSI (MSI-H) and microsatellite stable (MSS), respectively) are balanced according to their occurrence in the clinics, which is ~15% of MSI-H and 85% of MSS cases for our highlighted use case in CRC.

In a binary classification task such as predicting the presence or absence of a specific biomarker (e.g., MSI-H or MSS), the trained deep learning model generates an output probability between 0 and 1 for the prediction of the WSI. This output probability can be interpreted as the model’s confidence in predicting the positive class (e.g., MSI-H) and is subsequently used for calculation of the model’s performance metrics (Fig. 5). Specifically, the model evaluates each patch individually by assigning a score based on its feature vector, which indicates the patch’s importance or informative value for making the final prediction of the entire WSI. Patches with higher scores are considered more influential in the model’s decision-making process. The patches with the highest scores are identified as top tiles, representing the regions of the image that have the most significant impact on the model’s prediction (see Step 21). These scores can be visualized as a heatmap, in which the color intensity represents the importance of each patch, highlighting the most influential regions of the image for the model’s prediction (Fig. 6).

Fig. 5: Anticipated results of the evaluation phase of the protocol for the analysis of CRC from TCGA and the CPTAC.
figure 5
a,b Area under the receiver-operator characteristic (AUROC) (a) and area under the precision-recall characteristic (AUPRC) (b) curves to evaluate the performance of fivefold cross-validation for MSI-H trained on TCGA-CRC. c,d AUROC (c) and AUPRC (d) curves to evaluate the performance of the final model trained and deployed on the external validation cohort CPTAC-CRC. The bootstrapped 95% confidence interval on the external dataset is depicted in light blue. The dotted red line signifies the performance of a random prediction model.

Source data

Full size image

Fig. 6: Anticipated results of the translation phase of the protocol for the analysis of CRC from the CPTAC.
figure 6
a, The heatmap and top tiles for CPTAC slide 20CO003-2d266d2d-4bb3-436c-a3e7-06acb2, labeled as ‘MSI-H’ for the ground truth. Gradient class-activation maps (gradCAM) are used to highlight the top influential tiles in the model’s decision making for predicting either MSI-H or MSS. In a binary prediction setting as showcased, the heatmaps are inversely correlated: the more a tile positively attributes to MSI-H (colored red), the more it negatively attributes to MSS (colored blue). b,c Kaplan-Meier curves for stage I, II and III CRC patients from the CPTAC cohort. Stage IV patients are not analyzed separately because only one datapoint with disease-free survival was available in the CPTAC cohort. Difference in survival between the stratified risk groups is measured by using the log-rank test. A P < 0.05 from the log-rank test indicates that there is a statistically significant difference in survival between the stratified risk groups.

Source data

Full size image

In computational pathology, the complexity of a problem can be determined by the level of visual abstraction required for its analysis (see Step 22). Simpler problems in computational pathology typically involve tasks that a pathologist can visually identify and quantify, such as the detection of tumor presence. On the other hand, complex problems in computational pathology involve tasks that require the identification of nuanced phenotypes and their association with genetic alterations. In layperson terms, if a pathologist can easily identify it by observing the slide, then a deep learning model can probably easily identify it as well, given enough training data.

Materials
Equipment
Data
Histopathology slides. The slides for TCGA are available at https://portal.gdc.cancer.gov/. The slides for CPTAC are available at https://proteomics.cancer.gov/data-portal

Genomics data from TCGA and CPTAC. The molecular and clinical data for TCGA and CPTAC used in the experiments are available at https://github.com/KatherLab/cancer-metadata

CAUTION

The use of anonymized patient samples may need ethical approval. We examined anonymized patient samples from several academic institutions in this investigation. CPTAC and TCGA did not require formal ethics approval for a retrospective study of anonymized samples. The overall analysis was approved by the Ethics Commission of the Medical Faculty of the Technical University Dresden (BO-EK-444102022).

Hardware
Preprocessing is the heaviest operation in the protocol and requires a large amount of random-access memory (RAM), depending on the size of the WSI. The entire process can be run on CPUs, requiring a minimum of ~50 GB of system RAM, being strongly dependent on the WSI size and the chosen ratio of microns per pixel (MPP). No graphics processing units (GPUs) are necessarily required to run STAMP, although substantial speed improvements can be made by using GPUs. If a GPU is used in preprocessing, ≥10 GB of video RAM is necessary, whereas a GPU used in modeling requires upwards of 40 GB of video RAM because of the computationally demanding transformer architecture

Software
The Python software packages for the protocol can be installed all at once by using the STAMP installer (Box 1), as also described in the README.md file (https://github.com/KatherLab/STAMP). The required software is listed in the pyproject.toml file and is intended for usage in Python 3.10 or higher (Supplementary Table 1). Note that these packages are the key building blocks and are dependent on other supportive packages, which will trigger a series of installations to satisfy the dependencies. Therefore, this list is not an exhaustive overview of every package but can be obtained by using the CLI commands conda list or pip list inside your environment. The configuration for the entire protocol is set by the user in the configuration file, config.yaml (https://github.com/KatherLab/STAMP/blob/main/stamp/config.yaml). The STAMP workflow has been developed for the Ubuntu distribution of Linux operating systems but can be used on any operating system using the provided containerized environment on GitHub

Box 1 Installation instructions for the STAMP software
Installation steps
1.
Install OpenSlide. Use the following command or refer to the official installation instructions (https://openslide.org/download/#distribution-packages):

apt update & & apt install -y openslide-tools libgl1-mesa-glx

2.
Install Conda. Follow the steps outlined in the Conda installation guide found at https://conda.io/projects/conda/en/latest/user-guide/install/index.html to set up Conda on your local machine. Create an environment with Python 3.10 and activate it:

conda create -n stamp python=3.10
conda activate stamp
conda install -c conda-forge libstdcxx-ng=12

3.
Install the STAMP package. Use pip to install the STAMP package from the GitHub repository: https://github.com/KatherLab/STAMP.

pip install git+https://github.com/KatherLab/STAMP

4.
Initialize STAMP. After installation, obtain the required configuration in your current working directory by running the command:

stamp init

5.
Download the required resources. After initialization, download essential resources like the CTransPath feature extractor’s weights by running the command:

stamp setup

Upon successful installation, the stamp command can be used in the command line interface.

Show more
Procedure
CRITICAL

The software was developed to involve as little additional programming or scripting for the user as possible. Consequently, the user can fill in all paths, variables and settings in the configuration file, which will serve as the configuration for each step described below that requires interaction with the STAMP software (Box 2). The configuration file should be saved after every change applied to it before running the STAMP CLI commands.

Box 2 Overview of all CLI commands of the STAMP software
stamp init

: Create a new configuration file in the current directory

stamp setup

: Download the required resources

stamp config

: Show the configuration settings

stamp preprocess

: Preprocess WSIs and extract features

stamp crossval

: Train n_splits models by using cross-validation

stamp train

: Train a single model on the entire training cohort

stamp deploy

: Deploy a trained model on an external testing cohort

stamp statistics

: Compute the AUROC, AUPRC and corresponding 95% CI metrics

stamp heatmaps

: Generate heatmaps and corresponding top tiles

Show more
Orchestration: formal problem definition
Timing 1–2 weeks hands on, 4–12 weeks total

1.
Define the clinical use case. Define the clinical hypothesis to be tested and the target to be predicted. The pipeline for this protocol is intended for weakly supervised problems only. Patient-level biomarker labels are used for training the model, without any annotations of the WSI. The main use case of this protocol is the detection of biomarkers directly from WSIs.

2.
Assess the quantity of data. Identify how many patients with corresponding WSIs and biomarker labels are available. A patient can have multiple H&E-stained slides.

3.
Define the inclusion and exclusion criteria. Visually evaluate the quality of the WSIs by using a pathology slide viewer of choice, such as QuPath44. It is important to identify slides that have tumor tissue, contain pen markings and are either a biopsy or resection (Fig. 3). These observations can be done on the thumbnail level and thus do not require the full resolution slide, which allows for rapid iteration. The metadata of the WSIs should contain the MPP value, which is used for the digital loading and processing of the slides. Define which patients and corresponding slides are used for subsequent analysis on the basis of the data quality. Specifically, use quality measures regarding the availability of the target label, the presence of dirt and pen markings, the presence of tumor tissue, the type of surgical sample (biopsy or resection), the type of tissue (primary tumor or metastasis) and the availability of the MPP in the WSIs’ metadata23. The visual evaluation of the WSIs quality is time intensive but is a crucial step in ensuring that the model sees the highest quality of data during training. Therefore, WSIs of lower quality should not be used for training the model but could be used for testing the model’s robustness on real-world scenarios with sub-optimal quality instead. Technically, there is no upper limit for the amount of slides to be included. However, the user should consider the effort needed to visually assess large cohorts and establish a manageable limit for an initial proof-of-concept study with manually curated data. Randomly selecting 50 slides from a cohort is recommended to assess the overall quality, which then determines the level of detail required for further visual scrutiny of the entire cohort.

4.
Determine the desired slide resolution. Define which slide resolution is required for the use case defined in Step 1. The slides are usually scanned at 40× or 20× magnification, approximately corresponding to an MPP value of 0.25 and 0.5, respectively. The magnification-to-MPP ratio is scanner dependent. The pipeline allows for downscaling to lower resolutions but does not allow for upscaling to resolutions beyond the highest available representation in the pyramidal slide files’ metadata. With the default values, the analyzed tile resolutions are equivalent to ~10× magnification (1.14 MPP). Empirical evidence in weakly supervised computational pathology does not show clear performance benefits with magnifications beyond 10×. Therefore, starting with 10× magnification is recommended because of the substantially reduced computational burden from handling fewer patches per slide but can be iteratively increased or decreased depending on the clinical hypothesis.

5.
Transform the data. First, decide whether the biomarkers being predicted require transformations before modeling on the basis of Steps 1–4. Transformations could include the removal or merging of categories within a biomarker, binning continuous targets or merging several biomarkers. Second, decide whether the WSIs should be stain normalized58. Stain normalization is commonly used for H&E-stained slides originating from different hospitals, applied to reduce batch effects59. Empirical evidence shows that performing stain normalization by default is good practice but might become obsolete with the rise of foundation models for pathology42. The transformations of the biomarker data make the subsequent analysis more manageable, because this is a way of introducing domain knowledge that discards abundant or contradicting information before it is seen by an algorithm.

6.
Define the validation strategy. First, establish the patient allocation strategy to validate the hypothesis defined in Step 1. The validation strategy requires the inclusion and exclusion criteria defined in Step 3 to identify the number of patients available for the internal training cohort and external testing cohort. Following best scientific practice in the field of computational pathology and machine learning, this protocol assumes the availability of an external cohort that can be used for testing the model’s generalizability. Second, define the statistical tests and metrics that enable testing of the hypothesis. Establish clear criteria and define value ranges for metrics, as well as specify the corresponding statistical tests, to either accept or reject the hypothesis. This includes the expected presence of certain cells or genomic alterations predominantly belonging to the categories of the biomarker. Finally, performing power calculations is crucial to ensure that evaluations like survival analysis are adequately powered. Accurate sample size estimation is essential to avoid misleading results, especially when dealing with limited observations.

Execution: data preprocessing
Timing 2–6 h hands on, 1–6 weeks total

7.
Set up the computing resources. First, follow the installation instructions and finalize the installation of the STAMP workflow (Box 1). Second, ensure that a configuration file, such as the aforementioned config.yaml example file, exists (see Equipment, Software). Next, make the appropriate computing resources available for preprocessing the WSI data as indicated in Hardware. Preprocessing of a WSI for deep learning purposes is a data- and computationally intensive procedure. The computational demand is dictated by the size of the gigapixel WSI, as well as the chosen parameters for the MPP (defined in Step 4). Reducing the MPP leads to a higher resolution, resulting in a larger amount of tiles, which heavily increases the computational resources needed. The CLI command stamp config can be executed to print the current configuration that will be used by the STAMP software.

8.
Set up the preprocessing configuration. Open the configuration file and insert the required arguments for the preprocessing section (Table 2). The predefined settings for preprocessing are recommended to be kept as the default values to reproduce the experimental results.

Table 2 Preprocessing configuration description in the config.yaml file
Full size table

9.
Extract features from the WSI. Run the preprocessing pipeline by using the configuration from Step 8 with the CLI command stamp preprocess. This triggers a chain of processes that loads the WSIs into memory at a magnification defined by the MPP, tessellates the slide into n tiles of 224 × 224 pixels, removes tiles that contain too little tissue or are blurry through Canny edge detection60, optionally normalizes the H&E staining color distribution of the n tiles according to Macenko’s method58, runs inference with the feature extractor model on each tile to obtain its feature vector and then concatenates the feature vectors of all n tiles of one slide into one large feature matrix. By default, the preprocessing uses a robust42 model that has been trained on 14 million patches from 32,000 publicly available slides across various cancer types, CTransPath40. The feature matrix for each slide has a dimensionality of n × 768 for CTransPath, where the n tiles vary per slide. The preprocessing step has two key outputs: the intermediate products of the WSIs and a feature vector file. The intermediate products are images that can be visually inspected to determine the correct behavior of the preprocessing procedure (see Step 10). The extracted features are the final product, which is used later in the modeling section to build a classification deep learning model.

TROUBLESHOOTING

10.
Assess image preprocessing. Analyze the intermediate products of the preprocessing pipeline, which are stored in the directory given as input for the cache_dir argument from the preprocessing section in the configuration file. First, observe the CLI and identify whether the script has successfully run. A successful run finishes with a summary regarding the number of slides that were processed and the total runtime of the preprocessing. Second, scroll through a sample of the folders named after the WSI that was analyzed and perform quality control of the files within each folder: slide.jpg, canny_slide.jpg and norm_slide.jpg. These image files contain the WSI loaded at the desired MPP, the partially preprocessed image after rejecting blurry and background tiles and the fully preprocessed image with stain-normalized tiles, respectively. Each non-rejected tile present in the norm_slide.jpg file will result in a feature vector that will be concatenated into a feature matrix for the entire slide. This protocol allows for non-tumor-containing tiles to be kept, because the downstream classification tasks learn relevant from non-relevant tiles when provided with sufficient samples, including pen marks32. It is, however, recommended to use slides for which non-essential information is kept to a minimum. Note that image features of a cohort only have to be extracted once for each desired resolution and can be reused for other downstream classification tasks.

Execution: modeling
Timing 3–9 h hands on, 1–3 weeks total

11.
Define the slide table. Create a table that links a pseudonymized patient identifier to the filename of the extracted slide features23. The protocol accepts the presence of multiple slides, and therefore slide features, per patient. In the multi-slide case, all feature vectors belonging to a single patient are concatenated and treated as a single instance. Moreover, the protocol allows for modeling of slide-specific labels, treating every slide as a separate instance. Usage of aforementioned functionality is dependent on the design of the slide table (Box 3).

12.
Define the clinical table. Create a table that links a pseudonymized patient identifier to the biomarker data23 (Box 4).

13.
Define the data splits. Determine how the data should be split for modeling (Box 5). The default splitting mechanism is fivefold cross-validation (Supplementary Fig. 1) and a single 80-20 split for the final model training.

14.
Set up the modeling configuration. Open the configuration file and insert the required arguments for the modeling section (Table 3). The arguments in the modeling section are used for cross-validation, final model training and deployment of the model on an external cohort, depending on the CLI STAMP command that is used. At this step, provide the arguments required for cross-validation.

Table 3 Modeling configuration description in the config.yaml file
Full size table

15.
Train cross-validated models. Run the modeling pipeline with cross-validation by using the configuration from Step 14 with the CLI command stamp crossval. By default, cross-validated training uses the fivefold data splits described in Step 13. This process results in the creation of five distinct models, all built with the same architecture but exhibiting differences in their learned parameters because of the diverse training data. This allows the model architecture performance to be measured under various data conditions.

TROUBLESHOOTING

16.
Evaluate cross-validation. Open the configuration file and insert the required arguments for the statistics section (Table 4). Measure the performance of the cross-validated models by using the CLI command stamp statistics. The area under the receiver operator characteristic (AUROC) and the area under the precision recall characteristic (AUPRC) are two computed metrics that are, preferably, as close to 1 as possible, indicating the quality of the classification. Simultaneously, the 95% confidence interval (CI) is calculated; a small 95% CI indicates greater robustness in performance across the various cross-validated models (Fig. 5a,b). Note that additional metrics defined in Step 6 can be calculated manually by using the patient predictions and are not included in the STAMP workflow by default.

Table 4 Statistics configuration description in the config.yaml file
Full size table

TROUBLESHOOTING

17.
Train the final model. Open the configuration file and insert the required arguments for the modeling section (Table 3). At this step, provide the arguments required for full training. Choosing a different output directory for the output argument in the modeling section is suggested, to not interfere with the results from the cross-validation in Steps 15 and 16. Run the modeling pipeline to train the final model by using the CLI command stamp train. This process results in the creation of a single model file (.pkl) that has been trained on 100% of the cohort, thus requiring an external cohort to measure its performance in predicting the biomarker.

TROUBLESHOOTING

Box 3 Example of a slide table used in the STAMP protocol
The slide table is a file (.xlsx or .csv) that contains two columns, a PATIENT column, containing the pseudonymized patient identifiers as string, and the FILENAME column, containing the feature matrix names as string without the file extension (.h5). A pseudonymized patient identifier can be linked to multiple WSIs by repeating the pseudonymized patient identifier and adding the additional corresponding feature matrix names. In the downstream classification task, the features of all the slides belonging to a single patient will be concatenated into one big feature matrix during runtime. This is wished-for behavior for modeling a patient-level target in homogeneous tumors, such as a genetic alteration, which might contain valuable information in all the observed slides. However, when modeling a slide-specific target, such as a pathologist-assigned score, it is recommended to treat the WSIs as individual instances and not concatenate their features in the downstream classification task. In this case, each slide should have a unique pseudonymized patient identification label to avoid concatenation of slide features during runtime.

PATIENT

FILENAME

ID_1337

slide_ID_1337_1

ID_1337

slide_ID_1337_2

ID_1999

slide_ID_1999_1

ID_1608

slide_ID_1608_1

Show more
Box 4 Example of a clinical table used in the STAMP protocol
The clinical table is a file (.xlsx or .csv) that contains at least two columns, one column with the pseudonymized patient identifiers, PATIENT, and a column with the biomarker data on which to train the model, which is a string with any chosen name without special characters. The clinical table can have other columns, as long as each column name is a unique string. The absolute path of the clinical table is used as input for the clini_table argument in the modeling section of the configuration file. To specify the biomarker to be modeled, provide the name of the corresponding column name in the clinical table as a string input for the target_label argument. The protocol supports multimodality by adding categorical and continuous tabular variables to the extracted feature vector from the image32, by using the cat_variable and cont_variable arguments, respectively. Except for the PATIENT column, the column names in the clinical table that are not explicitly provided as an argument are not used for modeling. Defining the desired categories on which to train by using the categories argument in the modeling section is recommended. Missing values in the clinical table should be empty cells, because strings or characters, like ‘NA’, ‘NaN’, ‘None’ or ‘-’, will be seen as categories, unless the categories are explicitly defined by using the categories argument.

PATIENT

isMSIH

ID_1337

MSI-H

ID_1999

MSS

ID_1608

 
ID_0311

MSI-H

Show more
Box 5 Data splitting mechanisms used in the STAMP protocol
There are four main data components that are required for proper development of a model to predict biomarkers directly from WSI: training data, validation data, test data and external test data. Training and validation data are seen by the model during training time, which is then evaluated by using the unseen test data. Aforementioned data splits are usually from the same patient cohort. To test generalizability of the model, its performance is measured on the unseen external test set. By default, fivefold cross-validation is performed on the cohort, splitting the provided clinical table from Step 12 into five permutations of an 80-20 split for modeling (using training and validation data) and testing data. Consequently, 64% of the provided cohort is used for training, 16% for validation and 20% for testing, repeated for five iterations, stratified by the target variable (Supplementary Fig. 1). In the multi-slide case for patient-level labels, all feature vectors belonging to a single patient are concatenated and treated as a single instance. Consequently, the slide features belonging to a single patient can exist in only the training, validation or testing test, thus avoiding data leakage during training. In the multi-slide case for slide-specific labels, having an external validation cohort is of utmost importance, because different slides from the same patient could be present in the training, validation and test set, causing a form of data leakage that leads to overly optimistic performance estimates.After fivefold cross-validation, another training run is performed on the entire cohort to yield a single model. Again, the data are split 80-20, but this time solely for training and validation, because the external test set will be used to measure performance instead. This data splitting mechanism has been chosen because of its consistent usage for modeling in computational pathology in previous studies26,70,71. The external test set is unused and unseen during model training, to avoid data leakage. If no external test set is available, it is recommended to create pseudo-external test sets from the training cohort, ensuring that patients’ slides and hospital of origin are mutually exclusive in the training and testing cohort to reduce confounding factors during the modeling59.

Show more
Execution: evaluation
Timing 1–2 h hands on, 1–7 d total

18.
Set up the external validation configuration. Open the configuration file and insert the required arguments for the modeling section (Table 3). At this step, provide the arguments required for model deployment on an external cohort. Repeat Steps 7–10 to preprocess the external cohort and obtain the feature vectors, which are used for validation of the final model. Repeat Steps 11 and 12 to obtain the corresponding slide and clinical table for the external cohort.

19.
Deploy the final model. Run the modeling pipeline to deploy the final model on an external cohort by using the configuration from Step 18 with the CLI command stamp deploy. When deploying the model onto new data, the model is not trained or updated. The output of the model deployment on the external cohort is a single file containing the predictions for each patient.

TROUBLESHOOTING

20.
Evaluate the final model. Obtain the metrics and statistics for the final model’s performance on the external cohort. First, update the configuration file with the single file with patient predictions (Table 4). Second, use the CLI command stamp statistics. The predictions undergo bootstrapping through case resampling 1,000 times to recalculate the metrics, resulting in a 95% CI for the modeling performance of the external validation cohort (see Step 22; Fig. 5c,d).

TROUBLESHOOTING

21.
Generate a spatial interpretability map. Use the final model to generate a prediction heatmap. First, inspect the patient prediction file used in Step 20 to calculate the metrics. Here, the prediction scores for each patient are listed and ranked by loss; the most accurate predictions are at the top of the file. Second, select patients from which to analyze the spatial interpretability map. Choosing patients with an extreme value of the prediction score (i.e., the scores closest to 1) is recommended. A more extreme prediction score value indicates that a stronger signal was captured by the model, which helps with human interpretability of the heatmaps. Third, select the slide names belonging to the patients, observed in the slide table file from Step 18. Open the configuration file and insert the required arguments for the heatmaps section (Table 5). Generate the slide heatmaps by using the CLI command stamp heatmaps. This results in a heatmap showing the spatial relationship of the most important tiles for the model’s slide-level prediction, adding interpretability to the model’s decision making (Fig. 6a). Moreover, the desired amount of influential tiles for decision making is stored as separate image files with the score and coordinates in the naming.

Table 5 Heatmaps configuration description in the config.yaml file
Full size table

TROUBLESHOOTING

22.
Review the model’s technical performance. First, investigate the model’s performance from the metrics and statistics calculated in Step 20. The AUROC and AUPRC with 95% CI are desired to be as close to 1 within the range of [0,1], with the 95% CI not crossing the point of insignificance. The point of insignificance lies at 0.5 for the AUROC and is dependent on the data distribution of the target label for the AUPRC, indicating that the predictions are random when crossed. An acceptable performance is dependent on the problem defined in Steps 1–6, where predicting MSI status is known to be accurately predicted15,32,33, whereas predicting survival is a relatively more complex problem to model61,62, which is reflected in the performance metrics. Second, assess whether the model pays attention to parts of the WSI that contain tissue and that it does not focus on dirt, pen marks or other artefacts present on the slide after preprocessing. If the model does focus on parts of the WSI that are not related to the tissue, the data need to be better curated (repeat Steps 2–5 iteratively), or the model requires more samples to avoid overfitting.

Interpretation: translation
Timing 1–2 weeks hands on, 2–6 weeks total

23.
Conduct concordance analysis. Use the final model’s predicted scores of the external cohort to measure concordance with clinicopathological and biological variables that are relevant to testing the hypothesis as defined in Steps 1 and 6. The aim is to find patterns in the model predictions that align with readily known clinical concepts, which hint towards the model’s capability of learning relevant biology for its predictions. In theory, however, these efforts could lead to new knowledge about biological patterns that are not yet supported by other sources. It is a standard procedure to calculate correlations between a predicted biomarker and other variables relating to biological processes20,54, and therefore software for performing a concordance analysis is not included in the STAMP workflow.

24.
Review the model’s clinical utility. Let a pathologist review the spatial interpretability map and corresponding tiles that influenced the model’s decision making from Step 21. Link the findings of the pathologist and the concordance analysis in Step 23 to literature of prior studies to discover if the developed biomarker is in accordance with known biological and medical concepts21,33. Finally, the guidelines defined by STARD (Standards for Reporting of Diagnostic Accuracy Studies)-AI63 or TRIPOD (Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis)-AI64 can be used to formally evaluate and report the model’s clinical utility.

25.
Calculate prognostic capability. Use the labels predicted for the external cohort by the final model to stratify risk groups. Visualize the stratification of risk groups by using Kaplan Meier curves (Fig. 6b,c). The predicted labels are used to separate patient groups for which the survival variable is plotted. Calculate the statistical significance of different survival between the stratified risk groups by using the log-rank test. Use the predicted labels of the external cohort by the final model and covariates (such as tumor stage, age and sex) to perform the multivariable Cox proportional-hazards test. This yields a hazard ratio (HR) and corresponding P value. Statistical significance of the HR is achieved when the 95% CI does not overlap with HR = 1. Models that produce HR values further away from 1 indicate a more pronounced effect and are considered to have greater prognostic value. It is a standard procedure to stratify groups and calculate the differences in survival risk9,29,34, and therefore the software for calculating prognostic capabilities is not included in the STAMP workflow.

Troubleshooting
The most common troubleshooting in this protocol is software related. Usage of the software in the provided container minimizes the risk of faulty installations, which eliminates software dependency–related bugs from causing unwanted behavior. Nonetheless, we have identified common scenarios from users around the world in which the code could not process the data as desired, and we have summarized those (Table 6). Unidentified errors in the code can be submitted to our issues page on GitHub (https://github.com/KatherLab/STAMP/issues) to further increase the robustness of the computational workflow.

Table 6 Troubleshooting table
Full size table

Timing
The estimated duration of the protocol is heavily reliant on the defined problem, the amount of samples and the available computational resources:

Steps 1–6, formal problem definition (lead: clinician): 1–2 weeks (hands-on execution), 4–12 weeks (total duration)

Steps 7–10, data preprocessing (lead: engineer): 2–6 h (hands-on execution), 1–6 weeks (total duration)

Steps 11–17, modeling (lead: engineer): 3–9 h (hands-on execution), 1–3 weeks (total duration)

Steps 18–22, evaluation (lead: engineer): 1–2 h (hands-on execution), 1–7 d (total duration)

Steps 23–25, translation (lead: clinician): 1–2 weeks (hands-on execution), 2–6 weeks (total duration)

For the clinician-led steps, it is assumed that a project idea is conceptualized from scratch but that data have been readily collected. For the engineer-led steps, it is assumed that a Linux system on which computations can be run has been readily set up.

Anticipated results
We illustrate our protocol with the phenotypic features of CRC WSIs mapped to the genotypic features of MMR genes related to MSI. After preprocessing the WSIs, 599 slides and 372 slides remained for the TCGA and CPTAC cohorts, respectively. For the actual training and deployment, which depends on the available slides and biomarker information, 444 patients were used for the internal training cohort TCGA, and 105 patients were used for the external validation cohort CPTAC.

Performing a fivefold cross-validation strategy for model exploration, the transformer models reached a mean AUROC and 95% CI of 0.84 (0.75–0.93) and a mean AUPRC and 95% CI of 0.58 (0.39–0.77) for the prediction of MSI-H (Fig. 5a,b). This indicates robust predictive capabilities of the model on the internal training cohort. However, to measure generalizability and robustness, the model should be deployed on an external cohort. Therefore, a single transformer model was trained on the full dataset of the TCGA training cohort, which was then deployed on the external CPTAC cohort. The deployed model showed accurate performance on the external CPTAC cohort, reaching an AUROC with bootstrapped 95% CI of 0.85 (0.74–0.94) and an AUPRC with bootstrapped 95% CI of 0.68 (0.55–0.87) (Fig. 5c,d). This indicates that the model learned generalizable features that perform similarly in both the internal and external test cohort. Note that the training procedure is of a stochastic nature, which might result in slightly different numbers when running experiments. The provided confidence intervals serve as a guideline for the expected range of results when running multiple experiments on the same data with the same settings and is in line with the reported results in the reference paper33. Using the predicted scores of the deployed model in the concordance analysis revealed that the fraction of genome altered was significantly lower among MSI-H–predicted tumors than among MSS-predicted tumors (P < 0.0001), which was in line with a key prior study65. Moreover, the concordance analysis showed that the mutation count and mutation rate of the MSI-H–predicted tumors was significantly higher than those of the MSS-predicted tumors (P < 0.0001), which is in accordance with the concept of MSI-H status being a pattern of hypermutation66. These correlations between MSI-H predictions and biological process signatures show indications of the model having learned clinically known concepts surrounding MSI status.

To further assess the model’s capability of having learned clinically relevant concepts, heatmaps of the external CPTAC-CRC cohort were generated. The top tiles indicate the key influential tiles that were used to predict MSI-H and MSS, with the model scoring the example case as 88% MSI-H and 12% MSS (Fig. 6a). A pathology resident in training, K.J.H., assessed that high-confidence predictions for MSI-H tiles tended to contain mucin and had a presence of signet rings, which are tissue characteristics predominantly associated with MSI-H status67. Moreover, these findings are in line with a prior study on a commercially available MSI status prediction model in computational pathology54, suggesting our model’s correct identification of the phenotype corresponding to an MSI-H tumor in CRC. Low confidence predictions, such as 12% for MSS, may include tissue in the top tiles that is considered diagnostically irrelevant and should be disregarded as having any potential diagnostic value.

To assess the prognostic value of the predicted biomarker, the CPTAC cohort was stratified by ground-truth and predicted MSI status with disease-free survival as the endpoint. The analysis was limited to CRC patients from stage I, II and III due to having only a single data point available for stage IV. From the 105 patients in the CPTAC-CRC cohort, 18 observations were deleted because of missing disease-free survival data, resulting in an analysis with n = 80 patients with 7 events in total. The stratification of stage I–III patients from CPTAC-CRC by using the ground-truth MSI status yielded no significant stratification (P = 0.80) (Fig. 6b), whereas the stratification using the predicted MSI status did yield a significant stratification (P = 0.04) (Fig. 6c), as evaluated through the log-rank test. When measuring the prognostic value of MSI status together with age, sex and tumor stage covariates, the covariates (P > 0.05) and ground-truth (P = 0.84) and predicted (P = 0.08) MSI status did not yield significant hazard ratios, as measured by the multivariable Cox proportional-hazards model. Note that the prognostic analysis serves as an example on open-source data to exhibit the utility of the predicted biomarkers but is limited because of the low amount of survival events. The low sample size is also a reason for the counter-intuitive risk-group assignment of the predicted MSI-H status, which is found to have a better prognosis in early-stage CRC68. Therefore, no conclusive clinical findings should be drawn from this example use case of the prognostic value assessment through the STAMP protocol.

These data show the successful application of the STAMP workflow for a clinically relevant research question. This protocol aims at accelerating biomarker discovery by democratizing the best practices in weakly supervised computational pathology, enabling research toward better precision oncology at scale.

Data availability
Histopathology slides and genomics data from TCGA and CPTAC were used to train and validate the models. The slides for TCGA are available at https://portal.gdc.cancer.gov/. The slides for CPTAC are available at https://proteomics.cancer.gov/data-portal. The molecular and clinical data for TCGA and CPTAC used in the experiments are available at https://github.com/KatherLab/cancer-metadata. Source data are provided with this paper.

Code availability
The open-source STAMP software for the implementation of the MSI experiments is available on GitHub (https://github.com/KatherLab/STAMP).